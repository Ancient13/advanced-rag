{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Enhancing RAG with Contextual Retrieval\n",
    "\n",
    "We will use an LLM to generate for each chunk and document a contextual sentence to improve its retrival accuracy and use in hybrid search.\n",
    "\n",
    "* Generate the context sentence.\n",
    "* Enrich the chunk embedding vectors with the context.\n",
    "* Create a topic database to be used in hybrid search.\n",
    "* Perform hybrid search to improve retrieval results."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Visual improvements\n",
    "\n",
    "We will use [rich library](https://github.com/Textualize/rich) to make the output more readable, and supress warning messages."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "from rich.pretty import pprint\n",
    "from rich.theme import Theme\n",
    "from rich.console import Console\n",
    "from rich.panel import Panel\n",
    "from rich.text import Text\n",
    "\n",
    "custom_theme = Theme({\n",
    "    \"repr.own\": \"bright_yellow\",            # Class names\n",
    "    \"repr.tag_name\": \"bright_yellow\",       # Adjust tag names which might still be purple\n",
    "    \"repr.call\": \"bright_yellow\",           # Function calls and other symbols\n",
    "    \"repr.str\": \"bright_green\",             # String representation\n",
    "    \"repr.number\": \"bright_red\",            # Numbers\n",
    "    \"repr.attrib_name\": \"bright_yellow\",    # Attribute names\n",
    "    \"repr.attrib_value\": \"bright_blue\"      # Attribute values\n",
    "})\n",
    "\n",
    "# Apply the theme and print the object with rich formatting\n",
    "\n",
    "console = Console(theme=custom_theme)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "import warnings\n",
    "\n",
    "# Suppress warnings\n",
    "warnings.filterwarnings('ignore')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Loading a complex dataset of documents\n",
    "\n",
    "We will load a complex dataset of scientific documents from Arxiv. Applying naive chunks on such documents will give poor results in RAG applications."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<pre style=\"white-space:pre;overflow-x:auto;line-height:normal;font-family:Menlo,'DejaVu Sans Mono',consolas,'Courier New',monospace\"><span style=\"color: #ffff00; text-decoration-color: #ffff00\">Dataset</span><span style=\"font-weight: bold\">({</span>\n",
       "    features: <span style=\"font-weight: bold\">[</span><span style=\"color: #00ff00; text-decoration-color: #00ff00\">'id'</span>, <span style=\"color: #00ff00; text-decoration-color: #00ff00\">'title'</span>, <span style=\"color: #00ff00; text-decoration-color: #00ff00\">'summary'</span>, <span style=\"color: #00ff00; text-decoration-color: #00ff00\">'source'</span>, <span style=\"color: #00ff00; text-decoration-color: #00ff00\">'authors'</span>, <span style=\"color: #00ff00; text-decoration-color: #00ff00\">'categories'</span>, <span style=\"color: #00ff00; text-decoration-color: #00ff00\">'comment'</span>, <span style=\"color: #00ff00; text-decoration-color: #00ff00\">'journal_ref'</span>, \n",
       "<span style=\"color: #00ff00; text-decoration-color: #00ff00\">'primary_category'</span>, <span style=\"color: #00ff00; text-decoration-color: #00ff00\">'published'</span>, <span style=\"color: #00ff00; text-decoration-color: #00ff00\">'updated'</span>, <span style=\"color: #00ff00; text-decoration-color: #00ff00\">'content'</span>, <span style=\"color: #00ff00; text-decoration-color: #00ff00\">'references'</span><span style=\"font-weight: bold\">]</span>,\n",
       "    num_rows: <span style=\"color: #ff0000; text-decoration-color: #ff0000\">2673</span>\n",
       "<span style=\"font-weight: bold\">})</span>\n",
       "</pre>\n"
      ],
      "text/plain": [
       "\u001b[93mDataset\u001b[0m\u001b[1m(\u001b[0m\u001b[1m{\u001b[0m\n",
       "    features: \u001b[1m[\u001b[0m\u001b[92m'id'\u001b[0m, \u001b[92m'title'\u001b[0m, \u001b[92m'summary'\u001b[0m, \u001b[92m'source'\u001b[0m, \u001b[92m'authors'\u001b[0m, \u001b[92m'categories'\u001b[0m, \u001b[92m'comment'\u001b[0m, \u001b[92m'journal_ref'\u001b[0m, \n",
       "\u001b[92m'primary_category'\u001b[0m, \u001b[92m'published'\u001b[0m, \u001b[92m'updated'\u001b[0m, \u001b[92m'content'\u001b[0m, \u001b[92m'references'\u001b[0m\u001b[1m]\u001b[0m,\n",
       "    num_rows: \u001b[91m2673\u001b[0m\n",
       "\u001b[1m}\u001b[0m\u001b[1m)\u001b[0m\n"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "from datasets import load_dataset\n",
    "\n",
    "dataset = load_dataset(\"jamescalam/ai-arxiv2\", split=\"train\")\n",
    "console.print(dataset)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "from semantic_router.encoders import OpenAIEncoder\n",
    "\n",
    "os.environ[\"OPENAI_API_KEY\"] = os.getenv(\"OPENAI_API_KEY\")\n",
    "\n",
    "encoder = OpenAIEncoder(name=\"text-embedding-3-small\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "from semantic_chunkers import StatisticalChunker\n",
    "import logging\n",
    "\n",
    "logging.disable(logging.CRITICAL)\n",
    "\n",
    "chunker = StatisticalChunker(\n",
    "    encoder=encoder,\n",
    "    min_split_tokens=100,\n",
    "    max_split_tokens=500,\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "chunks_0 = chunker(docs=[dataset[\"content\"][0]])\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<pre style=\"white-space:pre;overflow-x:auto;line-height:normal;font-family:Menlo,'DejaVu Sans Mono',consolas,'Courier New',monospace\"><span style=\"color: #ff0000; text-decoration-color: #ff0000\">4</span> <span style=\"color: #ff0000; text-decoration-color: #ff0000\">2</span> <span style=\"color: #ff0000; text-decoration-color: #ff0000\">0</span> <span style=\"color: #ff0000; text-decoration-color: #ff0000\">2</span> n a J <span style=\"color: #ff0000; text-decoration-color: #ff0000\">8</span> <span style=\"font-weight: bold\">]</span> G L . s c <span style=\"font-weight: bold\">[</span> <span style=\"color: #ff0000; text-decoration-color: #ff0000\">1</span> v <span style=\"color: #ff0000; text-decoration-color: #ff0000\">8</span> <span style=\"color: #ff0000; text-decoration-color: #ff0000\">8</span> <span style=\"color: #ff0000; text-decoration-color: #ff0000\">0</span> <span style=\"color: #ff0000; text-decoration-color: #ff0000\">4</span> <span style=\"color: #ff0000; text-decoration-color: #ff0000\">0</span> . <span style=\"color: #ff0000; text-decoration-color: #ff0000\">1</span> <span style=\"color: #ff0000; text-decoration-color: #ff0000\">0</span> <span style=\"color: #ff0000; text-decoration-color: #ff0000\">4</span> <span style=\"color: #ff0000; text-decoration-color: #ff0000\">2</span> : v i X r a # Mixtral of Experts Albert Q. Jiang, Alexandre \n",
       "Sablayrolles, Antoine Roux, Arthur Mensch, Blanche Savary, Chris Bamford, Devendra Singh Chaplot, Diego de las \n",
       "Casas, Emma Bou Hanna, Florian Bressand, Gianna Lengyel, Guillaume Bour, Guillaume Lample, LÃ©lio Renard Lavaud, \n",
       "Lucile Saulnier, Marie-Anne Lachaux, Pierre Stock, Sandeep Subramanian, Sophia Yang, Szymon Antoniak, Teven Le \n",
       "Scao, ThÃ©ophile Gervet, Thibaut Lavril, Thomas Wang, TimothÃ©e Lacroix, William El Sayed Abstract We introduce \n",
       "Mixtral 8x7B, a Sparse Mixture of Experts <span style=\"font-weight: bold\">(</span>SMoE<span style=\"font-weight: bold\">)</span> language model. Mixtral has the same architecture as Mistral 7B, \n",
       "with the difference that each layer is composed of <span style=\"color: #ff0000; text-decoration-color: #ff0000\">8</span> feedforward blocks <span style=\"font-weight: bold\">(</span>i.e. experts<span style=\"font-weight: bold\">)</span>. For every token, at each \n",
       "layer, a router network selects two experts to process the current state and combine their outputs. Even though \n",
       "each token only sees two experts, the selected experts can be different at each timestep. As a result, each token \n",
       "has access to 47B parameters, but only uses 13B active parameters during inference. Mixtral was trained with a \n",
       "context size of 32k tokens and it outperforms or matches Llama <span style=\"color: #ff0000; text-decoration-color: #ff0000\">2</span> 70B and GPT-<span style=\"color: #ff0000; text-decoration-color: #ff0000\">3.5</span> across all evaluated benchmarks. \n",
       "In particular, Mixtral vastly outperforms Llama <span style=\"color: #ff0000; text-decoration-color: #ff0000\">2</span> 70B on mathematics, code generation, and multilingual benchmarks.\n",
       "We also provide a model fine- tuned to follow instructions, Mixtral 8x7B â Instruct, that surpasses GPT-<span style=\"color: #ff0000; text-decoration-color: #ff0000\">3.5</span> Turbo, \n",
       "Claude-<span style=\"color: #ff0000; text-decoration-color: #ff0000\">2.1</span>, Gemini Pro, and Llama <span style=\"color: #ff0000; text-decoration-color: #ff0000\">2</span> 70B â\n",
       "</pre>\n"
      ],
      "text/plain": [
       "\u001b[91m4\u001b[0m \u001b[91m2\u001b[0m \u001b[91m0\u001b[0m \u001b[91m2\u001b[0m n a J \u001b[91m8\u001b[0m \u001b[1m]\u001b[0m G L . s c \u001b[1m[\u001b[0m \u001b[91m1\u001b[0m v \u001b[91m8\u001b[0m \u001b[91m8\u001b[0m \u001b[91m0\u001b[0m \u001b[91m4\u001b[0m \u001b[91m0\u001b[0m . \u001b[91m1\u001b[0m \u001b[91m0\u001b[0m \u001b[91m4\u001b[0m \u001b[91m2\u001b[0m : v i X r a # Mixtral of Experts Albert Q. Jiang, Alexandre \n",
       "Sablayrolles, Antoine Roux, Arthur Mensch, Blanche Savary, Chris Bamford, Devendra Singh Chaplot, Diego de las \n",
       "Casas, Emma Bou Hanna, Florian Bressand, Gianna Lengyel, Guillaume Bour, Guillaume Lample, LÃ©lio Renard Lavaud, \n",
       "Lucile Saulnier, Marie-Anne Lachaux, Pierre Stock, Sandeep Subramanian, Sophia Yang, Szymon Antoniak, Teven Le \n",
       "Scao, ThÃ©ophile Gervet, Thibaut Lavril, Thomas Wang, TimothÃ©e Lacroix, William El Sayed Abstract We introduce \n",
       "Mixtral 8x7B, a Sparse Mixture of Experts \u001b[1m(\u001b[0mSMoE\u001b[1m)\u001b[0m language model. Mixtral has the same architecture as Mistral 7B, \n",
       "with the difference that each layer is composed of \u001b[91m8\u001b[0m feedforward blocks \u001b[1m(\u001b[0mi.e. experts\u001b[1m)\u001b[0m. For every token, at each \n",
       "layer, a router network selects two experts to process the current state and combine their outputs. Even though \n",
       "each token only sees two experts, the selected experts can be different at each timestep. As a result, each token \n",
       "has access to 47B parameters, but only uses 13B active parameters during inference. Mixtral was trained with a \n",
       "context size of 32k tokens and it outperforms or matches Llama \u001b[91m2\u001b[0m 70B and GPT-\u001b[91m3.5\u001b[0m across all evaluated benchmarks. \n",
       "In particular, Mixtral vastly outperforms Llama \u001b[91m2\u001b[0m 70B on mathematics, code generation, and multilingual benchmarks.\n",
       "We also provide a model fine- tuned to follow instructions, Mixtral 8x7B â Instruct, that surpasses GPT-\u001b[91m3.5\u001b[0m Turbo, \n",
       "Claude-\u001b[91m2.1\u001b[0m, Gemini Pro, and Llama \u001b[91m2\u001b[0m 70B â\n"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "first_chunk = ' '.join(chunks_0[0][0].splits)\n",
    "console.print(first_chunk)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "True"
      ]
     },
     "execution_count": 18,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "from dotenv import load_dotenv\n",
    "\n",
    "load_dotenv()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {},
   "outputs": [],
   "source": [
    "import anthropic\n",
    "\n",
    "client = anthropic.Anthropic(\n",
    "    # This is the default and can be omitted\n",
    "    api_key=os.getenv(\"ANTHROPIC_API_KEY\"),\n",
    ")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {},
   "outputs": [],
   "source": [
    "DOCUMENT_CONTEXT_PROMPT = \"\"\"\n",
    "<document>\n",
    "{doc_content}\n",
    "</document>\n",
    "\"\"\"\n",
    "\n",
    "CHUNK_CONTEXT_PROMPT = \"\"\"\n",
    "Here is the chunk we want to situate within the whole document\n",
    "<chunk>\n",
    "{chunk_content}\n",
    "</chunk>\n",
    "\n",
    "Please give a short succinct context to situate this chunk within the overall document for the purposes of improving search retrieval of the chunk.\n",
    "Answer only with the succinct context and nothing else.\n",
    "\"\"\"\n",
    "\n",
    "def situate_context(doc: str, chunk: str) -> str:\n",
    "    response = client.beta.prompt_caching.messages.create(\n",
    "        model=\"claude-3-haiku-20240307\",\n",
    "        max_tokens=1024,\n",
    "        temperature=0.0,\n",
    "        messages=[\n",
    "            {\n",
    "                \"role\": \"user\", \n",
    "                \"content\": [\n",
    "                    {\n",
    "                        \"type\": \"text\",\n",
    "                        \"text\": DOCUMENT_CONTEXT_PROMPT.format(doc_content=doc),\n",
    "                        \"cache_control\": {\"type\": \"ephemeral\"} #we will make use of prompt caching for the full documents\n",
    "                    },\n",
    "                    {\n",
    "                        \"type\": \"text\",\n",
    "                        \"text\": CHUNK_CONTEXT_PROMPT.format(chunk_content=chunk),\n",
    "                    }\n",
    "                ]\n",
    "            }\n",
    "        ],\n",
    "        extra_headers={\"anthropic-beta\": \"prompt-caching-2024-07-31\"}\n",
    "    )\n",
    "    return response"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "metadata": {},
   "outputs": [],
   "source": [
    "chunk_context = situate_context(dataset[\"content\"][0], first_chunk)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<pre style=\"white-space:pre;overflow-x:auto;line-height:normal;font-family:Menlo,'DejaVu Sans Mono',consolas,'Courier New',monospace\"><span style=\"color: #ffff00; text-decoration-color: #ffff00\">PromptCachingBetaMessage</span><span style=\"font-weight: bold\">(</span>\n",
       "    <span style=\"color: #ffff00; text-decoration-color: #ffff00\">id</span>=<span style=\"color: #00ff00; text-decoration-color: #00ff00\">'msg_01U87paNMV7Q5odMtg4LpWD8'</span>,\n",
       "    <span style=\"color: #ffff00; text-decoration-color: #ffff00\">content</span>=<span style=\"font-weight: bold\">[</span>\n",
       "        <span style=\"color: #ffff00; text-decoration-color: #ffff00\">TextBlock</span><span style=\"font-weight: bold\">(</span>\n",
       "            <span style=\"color: #ffff00; text-decoration-color: #ffff00\">text</span>=<span style=\"color: #00ff00; text-decoration-color: #00ff00\">'This chunk introduces Mixtral 8x7B, a sparse mixture of experts language model that outperforms </span>\n",
       "<span style=\"color: #00ff00; text-decoration-color: #00ff00\">Llama 2 70B and GPT-3.5 on various benchmarks. It also describes the model architecture and the fine-tuned Mixtral </span>\n",
       "<span style=\"color: #00ff00; text-decoration-color: #00ff00\">8x7B - Instruct model.'</span>,\n",
       "            <span style=\"color: #ffff00; text-decoration-color: #ffff00\">type</span>=<span style=\"color: #00ff00; text-decoration-color: #00ff00\">'text'</span>\n",
       "        <span style=\"font-weight: bold\">)</span>\n",
       "    <span style=\"font-weight: bold\">]</span>,\n",
       "    <span style=\"color: #ffff00; text-decoration-color: #ffff00\">model</span>=<span style=\"color: #00ff00; text-decoration-color: #00ff00\">'claude-3-haiku-20240307'</span>,\n",
       "    <span style=\"color: #ffff00; text-decoration-color: #ffff00\">role</span>=<span style=\"color: #00ff00; text-decoration-color: #00ff00\">'assistant'</span>,\n",
       "    <span style=\"color: #ffff00; text-decoration-color: #ffff00\">stop_reason</span>=<span style=\"color: #00ff00; text-decoration-color: #00ff00\">'end_turn'</span>,\n",
       "    <span style=\"color: #ffff00; text-decoration-color: #ffff00\">stop_sequence</span>=<span style=\"color: #800080; text-decoration-color: #800080; font-style: italic\">None</span>,\n",
       "    <span style=\"color: #ffff00; text-decoration-color: #ffff00\">type</span>=<span style=\"color: #00ff00; text-decoration-color: #00ff00\">'message'</span>,\n",
       "    <span style=\"color: #ffff00; text-decoration-color: #ffff00\">usage</span>=<span style=\"color: #ffff00; text-decoration-color: #ffff00\">PromptCachingBetaUsage</span><span style=\"font-weight: bold\">(</span>\n",
       "        <span style=\"color: #ffff00; text-decoration-color: #ffff00\">cache_creation_input_tokens</span>=<span style=\"color: #ff0000; text-decoration-color: #ff0000\">0</span>,\n",
       "        <span style=\"color: #ffff00; text-decoration-color: #ffff00\">cache_read_input_tokens</span>=<span style=\"color: #ff0000; text-decoration-color: #ff0000\">12532</span>,\n",
       "        <span style=\"color: #ffff00; text-decoration-color: #ffff00\">input_tokens</span>=<span style=\"color: #ff0000; text-decoration-color: #ff0000\">584</span>,\n",
       "        <span style=\"color: #ffff00; text-decoration-color: #ffff00\">output_tokens</span>=<span style=\"color: #ff0000; text-decoration-color: #ff0000\">73</span>\n",
       "    <span style=\"font-weight: bold\">)</span>\n",
       "<span style=\"font-weight: bold\">)</span>\n",
       "</pre>\n"
      ],
      "text/plain": [
       "\u001b[93mPromptCachingBetaMessage\u001b[0m\u001b[1m(\u001b[0m\n",
       "    \u001b[93mid\u001b[0m=\u001b[92m'msg_01U87paNMV7Q5odMtg4LpWD8'\u001b[0m,\n",
       "    \u001b[93mcontent\u001b[0m=\u001b[1m[\u001b[0m\n",
       "        \u001b[93mTextBlock\u001b[0m\u001b[1m(\u001b[0m\n",
       "            \u001b[93mtext\u001b[0m=\u001b[92m'This chunk introduces Mixtral 8x7B, a sparse mixture of experts language model that outperforms \u001b[0m\n",
       "\u001b[92mLlama 2 70B and GPT-3.5 on various benchmarks. It also describes the model architecture and the fine-tuned Mixtral \u001b[0m\n",
       "\u001b[92m8x7B - Instruct model.'\u001b[0m,\n",
       "            \u001b[93mtype\u001b[0m=\u001b[92m'text'\u001b[0m\n",
       "        \u001b[1m)\u001b[0m\n",
       "    \u001b[1m]\u001b[0m,\n",
       "    \u001b[93mmodel\u001b[0m=\u001b[92m'claude-3-haiku-20240307'\u001b[0m,\n",
       "    \u001b[93mrole\u001b[0m=\u001b[92m'assistant'\u001b[0m,\n",
       "    \u001b[93mstop_reason\u001b[0m=\u001b[92m'end_turn'\u001b[0m,\n",
       "    \u001b[93mstop_sequence\u001b[0m=\u001b[3;35mNone\u001b[0m,\n",
       "    \u001b[93mtype\u001b[0m=\u001b[92m'message'\u001b[0m,\n",
       "    \u001b[93musage\u001b[0m=\u001b[93mPromptCachingBetaUsage\u001b[0m\u001b[1m(\u001b[0m\n",
       "        \u001b[93mcache_creation_input_tokens\u001b[0m=\u001b[91m0\u001b[0m,\n",
       "        \u001b[93mcache_read_input_tokens\u001b[0m=\u001b[91m12532\u001b[0m,\n",
       "        \u001b[93minput_tokens\u001b[0m=\u001b[91m584\u001b[0m,\n",
       "        \u001b[93moutput_tokens\u001b[0m=\u001b[91m73\u001b[0m\n",
       "    \u001b[1m)\u001b[0m\n",
       "\u001b[1m)\u001b[0m\n"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "console.print(chunk_context)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "metadata": {},
   "outputs": [],
   "source": [
    "second_chunk = ' '.join(chunks_0[0][1].splits)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "metadata": {},
   "outputs": [],
   "source": [
    "second_chunk_context = situate_context(dataset[\"content\"][0], second_chunk)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 27,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<pre style=\"white-space:pre;overflow-x:auto;line-height:normal;font-family:Menlo,'DejaVu Sans Mono',consolas,'Courier New',monospace\"><span style=\"color: #ffff00; text-decoration-color: #ffff00\">PromptCachingBetaMessage</span><span style=\"font-weight: bold\">(</span>\n",
       "    <span style=\"color: #ffff00; text-decoration-color: #ffff00\">id</span>=<span style=\"color: #00ff00; text-decoration-color: #00ff00\">'msg_01WgBrxDRVLdrPoPCTbWsiyt'</span>,\n",
       "    <span style=\"color: #ffff00; text-decoration-color: #ffff00\">content</span>=<span style=\"font-weight: bold\">[</span>\n",
       "        <span style=\"color: #ffff00; text-decoration-color: #ffff00\">TextBlock</span><span style=\"font-weight: bold\">(</span>\n",
       "            <span style=\"color: #ffff00; text-decoration-color: #ffff00\">text</span>=<span style=\"color: #00ff00; text-decoration-color: #00ff00\">'This chunk introduces Mixtral 8x7B, a sparse mixture of experts language model that outperforms </span>\n",
       "<span style=\"color: #00ff00; text-decoration-color: #00ff00\">Llama 2 70B and GPT-3.5 on most benchmarks. It describes the key architectural details of Mixtral, including its </span>\n",
       "<span style=\"color: #00ff00; text-decoration-color: #00ff00\">use of a sparse mixture-of-experts network, and mentions that the base and instruct models are released under the </span>\n",
       "<span style=\"color: #00ff00; text-decoration-color: #00ff00\">Apache 2.0 license.'</span>,\n",
       "            <span style=\"color: #ffff00; text-decoration-color: #ffff00\">type</span>=<span style=\"color: #00ff00; text-decoration-color: #00ff00\">'text'</span>\n",
       "        <span style=\"font-weight: bold\">)</span>\n",
       "    <span style=\"font-weight: bold\">]</span>,\n",
       "    <span style=\"color: #ffff00; text-decoration-color: #ffff00\">model</span>=<span style=\"color: #00ff00; text-decoration-color: #00ff00\">'claude-3-haiku-20240307'</span>,\n",
       "    <span style=\"color: #ffff00; text-decoration-color: #ffff00\">role</span>=<span style=\"color: #00ff00; text-decoration-color: #00ff00\">'assistant'</span>,\n",
       "    <span style=\"color: #ffff00; text-decoration-color: #ffff00\">stop_reason</span>=<span style=\"color: #00ff00; text-decoration-color: #00ff00\">'end_turn'</span>,\n",
       "    <span style=\"color: #ffff00; text-decoration-color: #ffff00\">stop_sequence</span>=<span style=\"color: #800080; text-decoration-color: #800080; font-style: italic\">None</span>,\n",
       "    <span style=\"color: #ffff00; text-decoration-color: #ffff00\">type</span>=<span style=\"color: #00ff00; text-decoration-color: #00ff00\">'message'</span>,\n",
       "    <span style=\"color: #ffff00; text-decoration-color: #ffff00\">usage</span>=<span style=\"color: #ffff00; text-decoration-color: #ffff00\">PromptCachingBetaUsage</span><span style=\"font-weight: bold\">(</span>\n",
       "        <span style=\"color: #ffff00; text-decoration-color: #ffff00\">cache_creation_input_tokens</span>=<span style=\"color: #ff0000; text-decoration-color: #ff0000\">0</span>,\n",
       "        <span style=\"color: #ffff00; text-decoration-color: #ffff00\">cache_read_input_tokens</span>=<span style=\"color: #ff0000; text-decoration-color: #ff0000\">12532</span>,\n",
       "        <span style=\"color: #ffff00; text-decoration-color: #ffff00\">input_tokens</span>=<span style=\"color: #ff0000; text-decoration-color: #ff0000\">290</span>,\n",
       "        <span style=\"color: #ffff00; text-decoration-color: #ffff00\">output_tokens</span>=<span style=\"color: #ff0000; text-decoration-color: #ff0000\">92</span>\n",
       "    <span style=\"font-weight: bold\">)</span>\n",
       "<span style=\"font-weight: bold\">)</span>\n",
       "</pre>\n"
      ],
      "text/plain": [
       "\u001b[93mPromptCachingBetaMessage\u001b[0m\u001b[1m(\u001b[0m\n",
       "    \u001b[93mid\u001b[0m=\u001b[92m'msg_01WgBrxDRVLdrPoPCTbWsiyt'\u001b[0m,\n",
       "    \u001b[93mcontent\u001b[0m=\u001b[1m[\u001b[0m\n",
       "        \u001b[93mTextBlock\u001b[0m\u001b[1m(\u001b[0m\n",
       "            \u001b[93mtext\u001b[0m=\u001b[92m'This chunk introduces Mixtral 8x7B, a sparse mixture of experts language model that outperforms \u001b[0m\n",
       "\u001b[92mLlama 2 70B and GPT-3.5 on most benchmarks. It describes the key architectural details of Mixtral, including its \u001b[0m\n",
       "\u001b[92muse of a sparse mixture-of-experts network, and mentions that the base and instruct models are released under the \u001b[0m\n",
       "\u001b[92mApache 2.0 license.'\u001b[0m,\n",
       "            \u001b[93mtype\u001b[0m=\u001b[92m'text'\u001b[0m\n",
       "        \u001b[1m)\u001b[0m\n",
       "    \u001b[1m]\u001b[0m,\n",
       "    \u001b[93mmodel\u001b[0m=\u001b[92m'claude-3-haiku-20240307'\u001b[0m,\n",
       "    \u001b[93mrole\u001b[0m=\u001b[92m'assistant'\u001b[0m,\n",
       "    \u001b[93mstop_reason\u001b[0m=\u001b[92m'end_turn'\u001b[0m,\n",
       "    \u001b[93mstop_sequence\u001b[0m=\u001b[3;35mNone\u001b[0m,\n",
       "    \u001b[93mtype\u001b[0m=\u001b[92m'message'\u001b[0m,\n",
       "    \u001b[93musage\u001b[0m=\u001b[93mPromptCachingBetaUsage\u001b[0m\u001b[1m(\u001b[0m\n",
       "        \u001b[93mcache_creation_input_tokens\u001b[0m=\u001b[91m0\u001b[0m,\n",
       "        \u001b[93mcache_read_input_tokens\u001b[0m=\u001b[91m12532\u001b[0m,\n",
       "        \u001b[93minput_tokens\u001b[0m=\u001b[91m290\u001b[0m,\n",
       "        \u001b[93moutput_tokens\u001b[0m=\u001b[91m92\u001b[0m\n",
       "    \u001b[1m)\u001b[0m\n",
       "\u001b[1m)\u001b[0m\n"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "console.print(second_chunk_context)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 42,
   "metadata": {},
   "outputs": [],
   "source": [
    "arxiv_id = dataset[0][\"id\"]\n",
    "refs = list(dataset[0][\"references\"].values())\n",
    "doc_text = dataset[0][\"content\"]\n",
    "title = dataset[0][\"title\"]\n",
    "\n",
    "corpus_json = []\n",
    "for i, chunk in enumerate(chunks_0[0]):\n",
    "    chunk_text = ' '.join(chunk.splits)\n",
    "    contextualized_text = situate_context(doc_text, chunk_text).content[0].text\n",
    "    corpus_json.append({\n",
    "        \"text\": f\"{chunk_text}\\n\\n{contextualized_text}\",\n",
    "        \"metadata\" : {\n",
    "            \"title\": title,\n",
    "            \"arxiv_id\": arxiv_id,\n",
    "            \"references\": refs\n",
    "        }\n",
    "    })"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 43,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<pre style=\"white-space:pre;overflow-x:auto;line-height:normal;font-family:Menlo,'DejaVu Sans Mono',consolas,'Courier New',monospace\"><span style=\"font-weight: bold\">[</span>\n",
       "    <span style=\"font-weight: bold\">{</span>\n",
       "        <span style=\"color: #00ff00; text-decoration-color: #00ff00\">'text'</span>: <span style=\"color: #00ff00; text-decoration-color: #00ff00\">'4 2 0 2 n a J 8 </span><span style=\"color: #00ff00; text-decoration-color: #00ff00; font-weight: bold\">]</span><span style=\"color: #00ff00; text-decoration-color: #00ff00\"> G L . s c </span><span style=\"color: #00ff00; text-decoration-color: #00ff00; font-weight: bold\">[</span><span style=\"color: #00ff00; text-decoration-color: #00ff00\"> 1 v 8 8 0 4 0 . 1 0 4 2 : v i X r a # Mixtral of Experts Albert Q. </span>\n",
       "<span style=\"color: #00ff00; text-decoration-color: #00ff00\">Jiang, Alexandre Sablayrolles, Antoine Roux, Arthur Mensch, Blanche Savary, Chris Bamford, Devendra Singh Chaplot, </span>\n",
       "<span style=\"color: #00ff00; text-decoration-color: #00ff00\">Diego de las Casas, Emma Bou Hanna, Florian Bressand, Gianna Lengyel, Guillaume Bour, Guillaume Lample, LÃ©lio </span>\n",
       "<span style=\"color: #00ff00; text-decoration-color: #00ff00\">Renard Lavaud, Lucile Saulnier, Marie-Anne Lachaux, Pierre Stock, Sandeep Subramanian, Sophia Yang, Szymon </span>\n",
       "<span style=\"color: #00ff00; text-decoration-color: #00ff00\">Antoniak, Teven Le Scao, ThÃ©ophile Gervet, Thibaut Lavril, Thomas Wang, TimothÃ©e Lacroix, William El Sayed </span>\n",
       "<span style=\"color: #00ff00; text-decoration-color: #00ff00\">Abstract We introduce Mixtral 8x7B, a Sparse Mixture of Experts </span><span style=\"color: #00ff00; text-decoration-color: #00ff00; font-weight: bold\">(</span><span style=\"color: #00ff00; text-decoration-color: #00ff00\">SMoE</span><span style=\"color: #00ff00; text-decoration-color: #00ff00; font-weight: bold\">)</span><span style=\"color: #00ff00; text-decoration-color: #00ff00\"> language model. Mixtral has the same </span>\n",
       "<span style=\"color: #00ff00; text-decoration-color: #00ff00\">architecture as Mistral 7B, with the difference that each layer is composed of 8 feedforward blocks </span><span style=\"color: #00ff00; text-decoration-color: #00ff00; font-weight: bold\">(</span><span style=\"color: #00ff00; text-decoration-color: #00ff00\">i.e. experts</span><span style=\"color: #00ff00; text-decoration-color: #00ff00; font-weight: bold\">)</span><span style=\"color: #00ff00; text-decoration-color: #00ff00\">.</span>\n",
       "<span style=\"color: #00ff00; text-decoration-color: #00ff00\">For every token, at each layer, a router network selects two experts to process the current state and combine their</span>\n",
       "<span style=\"color: #00ff00; text-decoration-color: #00ff00\">outputs. Even though each token only sees two experts, the selected experts can be different at each timestep. As a</span>\n",
       "<span style=\"color: #00ff00; text-decoration-color: #00ff00\">result, each token has access to 47B parameters, but only uses 13B active parameters during inference. Mixtral was </span>\n",
       "<span style=\"color: #00ff00; text-decoration-color: #00ff00\">trained with a context size of 32k tokens and it outperforms or matches Llama 2 70B and GPT-3.5 across all </span>\n",
       "<span style=\"color: #00ff00; text-decoration-color: #00ff00\">evaluated benchmarks. In particular, Mixtral vastly outperforms Llama 2 70B on mathematics, code generation, and </span>\n",
       "<span style=\"color: #00ff00; text-decoration-color: #00ff00\">multilingual benchmarks. We also provide a model fine- tuned to follow instructions, Mixtral 8x7B â Instruct, that </span>\n",
       "<span style=\"color: #00ff00; text-decoration-color: #00ff00\">surpasses GPT-3.5 Turbo, Claude-2.1, Gemini Pro, and Llama 2 70B â\\n\\nThis chunk introduces Mixtral 8x7B, a sparse </span>\n",
       "<span style=\"color: #00ff00; text-decoration-color: #00ff00\">mixture of experts language model that outperforms Llama 2 70B and GPT-3.5 on various benchmarks. It also describes</span>\n",
       "<span style=\"color: #00ff00; text-decoration-color: #00ff00\">the model architecture and the fine-tuned Mixtral 8x7B - Instruct model.'</span>,\n",
       "        <span style=\"color: #00ff00; text-decoration-color: #00ff00\">'metadata'</span>: <span style=\"font-weight: bold\">{</span><span style=\"color: #00ff00; text-decoration-color: #00ff00\">'title'</span>: <span style=\"color: #00ff00; text-decoration-color: #00ff00\">'Mixtral of Experts'</span>, <span style=\"color: #00ff00; text-decoration-color: #00ff00\">'arxiv_id'</span>: <span style=\"color: #00ff00; text-decoration-color: #00ff00\">'2401.04088'</span>, <span style=\"color: #00ff00; text-decoration-color: #00ff00\">'references'</span>: <span style=\"font-weight: bold\">[</span><span style=\"color: #00ff00; text-decoration-color: #00ff00\">'1905.07830'</span><span style=\"font-weight: bold\">]}</span>\n",
       "    <span style=\"font-weight: bold\">}</span>,\n",
       "    <span style=\"font-weight: bold\">{</span>\n",
       "        <span style=\"color: #00ff00; text-decoration-color: #00ff00\">'text'</span>: <span style=\"color: #00ff00; text-decoration-color: #00ff00\">'chat model on human bench- marks. Both the base and instruct models are released under the Apache </span>\n",
       "<span style=\"color: #00ff00; text-decoration-color: #00ff00\">2.0 license. Code: https://github.com/mistralai/mistral-src Webpage: https://mistral.ai/news/mixtral-of-experts/ # </span>\n",
       "<span style=\"color: #00ff00; text-decoration-color: #00ff00\">Introduction In this paper, we present Mixtral 8x7B, a sparse mixture of experts model </span><span style=\"color: #00ff00; text-decoration-color: #00ff00; font-weight: bold\">(</span><span style=\"color: #00ff00; text-decoration-color: #00ff00\">SMoE</span><span style=\"color: #00ff00; text-decoration-color: #00ff00; font-weight: bold\">)</span><span style=\"color: #00ff00; text-decoration-color: #00ff00\"> with open weights, </span>\n",
       "<span style=\"color: #00ff00; text-decoration-color: #00ff00\">licensed under Apache 2.0. Mixtral outperforms Llama 2 70B and GPT-3.5 on most benchmarks. As it only uses a subset</span>\n",
       "<span style=\"color: #00ff00; text-decoration-color: #00ff00\">of its parameters for every token, Mixtral allows faster inference speed at low batch-sizes, and higher throughput </span>\n",
       "<span style=\"color: #00ff00; text-decoration-color: #00ff00\">at large batch-sizes. Mixtral is a sparse mixture-of-experts network. It is a decoder-only model where the </span>\n",
       "<span style=\"color: #00ff00; text-decoration-color: #00ff00\">feedforward block picks from a set of 8 distinct groups of parameters. At every layer, for every token, a router </span>\n",
       "<span style=\"color: #00ff00; text-decoration-color: #00ff00\">network chooses two of these groups </span><span style=\"color: #00ff00; text-decoration-color: #00ff00; font-weight: bold\">(</span><span style=\"color: #00ff00; text-decoration-color: #00ff00\">the â\\n\\nThis chunk introduces Mixtral 8x7B, a sparse mixture of experts </span>\n",
       "<span style=\"color: #00ff00; text-decoration-color: #00ff00\">language model that outperforms Llama 2 70B and GPT-3.5 on most benchmarks. It describes the key architectural </span>\n",
       "<span style=\"color: #00ff00; text-decoration-color: #00ff00\">details of Mixtral, including its use of a sparse mixture-of-experts network, and mentions that the base and </span>\n",
       "<span style=\"color: #00ff00; text-decoration-color: #00ff00\">instruct models are released under the Apache 2.0 license.'</span>,\n",
       "        <span style=\"color: #00ff00; text-decoration-color: #00ff00\">'metadata'</span>: <span style=\"font-weight: bold\">{</span><span style=\"color: #00ff00; text-decoration-color: #00ff00\">'title'</span>: <span style=\"color: #00ff00; text-decoration-color: #00ff00\">'Mixtral of Experts'</span>, <span style=\"color: #00ff00; text-decoration-color: #00ff00\">'arxiv_id'</span>: <span style=\"color: #00ff00; text-decoration-color: #00ff00\">'2401.04088'</span>, <span style=\"color: #00ff00; text-decoration-color: #00ff00\">'references'</span>: <span style=\"font-weight: bold\">[</span><span style=\"color: #00ff00; text-decoration-color: #00ff00\">'1905.07830'</span><span style=\"font-weight: bold\">]}</span>\n",
       "    <span style=\"font-weight: bold\">}</span>\n",
       "<span style=\"font-weight: bold\">]</span>\n",
       "</pre>\n"
      ],
      "text/plain": [
       "\u001b[1m[\u001b[0m\n",
       "    \u001b[1m{\u001b[0m\n",
       "        \u001b[92m'text'\u001b[0m: \u001b[92m'4 2 0 2 n a J 8 \u001b[0m\u001b[1;92m]\u001b[0m\u001b[92m G L . s c \u001b[0m\u001b[1;92m[\u001b[0m\u001b[92m 1 v 8 8 0 4 0 . 1 0 4 2 : v i X r a # Mixtral of Experts Albert Q. \u001b[0m\n",
       "\u001b[92mJiang, Alexandre Sablayrolles, Antoine Roux, Arthur Mensch, Blanche Savary, Chris Bamford, Devendra Singh Chaplot, \u001b[0m\n",
       "\u001b[92mDiego de las Casas, Emma Bou Hanna, Florian Bressand, Gianna Lengyel, Guillaume Bour, Guillaume Lample, LÃ©lio \u001b[0m\n",
       "\u001b[92mRenard Lavaud, Lucile Saulnier, Marie-Anne Lachaux, Pierre Stock, Sandeep Subramanian, Sophia Yang, Szymon \u001b[0m\n",
       "\u001b[92mAntoniak, Teven Le Scao, ThÃ©ophile Gervet, Thibaut Lavril, Thomas Wang, TimothÃ©e Lacroix, William El Sayed \u001b[0m\n",
       "\u001b[92mAbstract We introduce Mixtral 8x7B, a Sparse Mixture of Experts \u001b[0m\u001b[1;92m(\u001b[0m\u001b[92mSMoE\u001b[0m\u001b[1;92m)\u001b[0m\u001b[92m language model. Mixtral has the same \u001b[0m\n",
       "\u001b[92marchitecture as Mistral 7B, with the difference that each layer is composed of 8 feedforward blocks \u001b[0m\u001b[1;92m(\u001b[0m\u001b[92mi.e. experts\u001b[0m\u001b[1;92m)\u001b[0m\u001b[92m.\u001b[0m\n",
       "\u001b[92mFor every token, at each layer, a router network selects two experts to process the current state and combine their\u001b[0m\n",
       "\u001b[92moutputs. Even though each token only sees two experts, the selected experts can be different at each timestep. As a\u001b[0m\n",
       "\u001b[92mresult, each token has access to 47B parameters, but only uses 13B active parameters during inference. Mixtral was \u001b[0m\n",
       "\u001b[92mtrained with a context size of 32k tokens and it outperforms or matches Llama 2 70B and GPT-3.5 across all \u001b[0m\n",
       "\u001b[92mevaluated benchmarks. In particular, Mixtral vastly outperforms Llama 2 70B on mathematics, code generation, and \u001b[0m\n",
       "\u001b[92mmultilingual benchmarks. We also provide a model fine- tuned to follow instructions, Mixtral 8x7B â Instruct, that \u001b[0m\n",
       "\u001b[92msurpasses GPT-3.5 Turbo, Claude-2.1, Gemini Pro, and Llama 2 70B â\\n\\nThis chunk introduces Mixtral 8x7B, a sparse \u001b[0m\n",
       "\u001b[92mmixture of experts language model that outperforms Llama 2 70B and GPT-3.5 on various benchmarks. It also describes\u001b[0m\n",
       "\u001b[92mthe model architecture and the fine-tuned Mixtral 8x7B - Instruct model.'\u001b[0m,\n",
       "        \u001b[92m'metadata'\u001b[0m: \u001b[1m{\u001b[0m\u001b[92m'title'\u001b[0m: \u001b[92m'Mixtral of Experts'\u001b[0m, \u001b[92m'arxiv_id'\u001b[0m: \u001b[92m'2401.04088'\u001b[0m, \u001b[92m'references'\u001b[0m: \u001b[1m[\u001b[0m\u001b[92m'1905.07830'\u001b[0m\u001b[1m]\u001b[0m\u001b[1m}\u001b[0m\n",
       "    \u001b[1m}\u001b[0m,\n",
       "    \u001b[1m{\u001b[0m\n",
       "        \u001b[92m'text'\u001b[0m: \u001b[92m'chat model on human bench- marks. Both the base and instruct models are released under the Apache \u001b[0m\n",
       "\u001b[92m2.0 license. Code: https://github.com/mistralai/mistral-src Webpage: https://mistral.ai/news/mixtral-of-experts/ # \u001b[0m\n",
       "\u001b[92mIntroduction In this paper, we present Mixtral 8x7B, a sparse mixture of experts model \u001b[0m\u001b[1;92m(\u001b[0m\u001b[92mSMoE\u001b[0m\u001b[1;92m)\u001b[0m\u001b[92m with open weights, \u001b[0m\n",
       "\u001b[92mlicensed under Apache 2.0. Mixtral outperforms Llama 2 70B and GPT-3.5 on most benchmarks. As it only uses a subset\u001b[0m\n",
       "\u001b[92mof its parameters for every token, Mixtral allows faster inference speed at low batch-sizes, and higher throughput \u001b[0m\n",
       "\u001b[92mat large batch-sizes. Mixtral is a sparse mixture-of-experts network. It is a decoder-only model where the \u001b[0m\n",
       "\u001b[92mfeedforward block picks from a set of 8 distinct groups of parameters. At every layer, for every token, a router \u001b[0m\n",
       "\u001b[92mnetwork chooses two of these groups \u001b[0m\u001b[1;92m(\u001b[0m\u001b[92mthe â\\n\\nThis chunk introduces Mixtral 8x7B, a sparse mixture of experts \u001b[0m\n",
       "\u001b[92mlanguage model that outperforms Llama 2 70B and GPT-3.5 on most benchmarks. It describes the key architectural \u001b[0m\n",
       "\u001b[92mdetails of Mixtral, including its use of a sparse mixture-of-experts network, and mentions that the base and \u001b[0m\n",
       "\u001b[92minstruct models are released under the Apache 2.0 license.'\u001b[0m,\n",
       "        \u001b[92m'metadata'\u001b[0m: \u001b[1m{\u001b[0m\u001b[92m'title'\u001b[0m: \u001b[92m'Mixtral of Experts'\u001b[0m, \u001b[92m'arxiv_id'\u001b[0m: \u001b[92m'2401.04088'\u001b[0m, \u001b[92m'references'\u001b[0m: \u001b[1m[\u001b[0m\u001b[92m'1905.07830'\u001b[0m\u001b[1m]\u001b[0m\u001b[1m}\u001b[0m\n",
       "    \u001b[1m}\u001b[0m\n",
       "\u001b[1m]\u001b[0m\n"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "console.print(corpus_json[:2])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Hybrid Search\n",
    "\n",
    "We will use bm25 supported database to complement the semantic search with the vector database."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 35,
   "metadata": {},
   "outputs": [],
   "source": [
    "import bm25s"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 44,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "cff8f569fdf546bab48470b86bec6726",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Split strings:   0%|          | 0/46 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "809015af9ef742e4aa7de65730d4e602",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "BM25S Count Tokens:   0%|          | 0/46 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "93e196ada3784714bf6f2d34df55a20a",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "BM25S Compute Scores:   0%|          | 0/46 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "corpus_text = [doc[\"text\"] for doc in corpus_json]\n",
    "\n",
    "# Tokenize the corpus and only keep the ids (faster and saves memory)\n",
    "corpus_tokens = bm25s.tokenize(corpus_text, stopwords=\"en\")\n",
    "\n",
    "# Create the BM25 retriever and attach your corpus_json to it\n",
    "retriever = bm25s.BM25(corpus=corpus_json)\n",
    "# Now, index the corpus_tokens (the corpus_json is not used yet)\n",
    "retriever.index(corpus_tokens)\n",
    "\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 45,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "ebdfdab1812b49858df0f3bb19dbee84",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Split strings:   0%|          | 0/1 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "ba52a4ca38514fb09bf5efa5f4d9e0fa",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "BM25S Retrieve:   0%|          | 0/1 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       "<pre style=\"white-space:pre;overflow-x:auto;line-height:normal;font-family:Menlo,'DejaVu Sans Mono',consolas,'Courier New',monospace\">Rank <span style=\"color: #ff0000; text-decoration-color: #ff0000\">1</span> <span style=\"font-weight: bold\">(</span>score: <span style=\"color: #ff0000; text-decoration-color: #ff0000\">2.11</span><span style=\"font-weight: bold\">)</span>: <span style=\"font-weight: bold\">{</span><span style=\"color: #00ff00; text-decoration-color: #00ff00\">'text'</span>: <span style=\"color: #00ff00; text-decoration-color: #00ff00\">\"Active Params French Arc-c HellaS MMLU German Arc-c HellaS MMLU Spanish Arc-c </span>\n",
       "<span style=\"color: #00ff00; text-decoration-color: #00ff00\">HellaS MMLU Italian Arc-c HellaS MMLU 33B 70B 13B 42.9% 65.4% 49.0% 39.3% 68.1% 49.9% 49.9% 72.5% 64.3% 49.4% 70.9%</span>\n",
       "<span style=\"color: #00ff00; text-decoration-color: #00ff00\">65.1% 58.2% 77.4% 70.9% 54.3% 73.0% 71.5% 55.4% 77.6% 72.5% 52.8% 75.1% 70.9% 41.1% 63.3% 48.7% 47.3% 68.7% 64.2% </span>\n",
       "<span style=\"color: #00ff00; text-decoration-color: #00ff00\">45.7% 69.8% 52.3% 50.5% 74.5% 66.0% Table 4: Comparison of Mixtral with Llama on Multilingual Benchmarks. On ARC </span>\n",
       "<span style=\"color: #00ff00; text-decoration-color: #00ff00\">Challenge, Hellaswag, and MMLU, Mixtral outperforms Llama 2 70B on 4 languages: French, German, Spanish, and </span>\n",
       "<span style=\"color: #00ff00; text-decoration-color: #00ff00\">Italian. # 3.2 Long range performance To assess the capabilities of Mixtral to tackle long context, we evaluate it </span>\n",
       "<span style=\"color: #00ff00; text-decoration-color: #00ff00\">on the passkey retrieval task introduced in </span><span style=\"color: #00ff00; text-decoration-color: #00ff00; font-weight: bold\">[</span><span style=\"color: #00ff00; text-decoration-color: #00ff00\">23</span><span style=\"color: #00ff00; text-decoration-color: #00ff00; font-weight: bold\">]</span><span style=\"color: #00ff00; text-decoration-color: #00ff00\">, a synthetic task designed to measure the ability of the model to </span>\n",
       "<span style=\"color: #00ff00; text-decoration-color: #00ff00\">retrieve a passkey inserted randomly in a long prompt. Results in Figure 4 </span><span style=\"color: #00ff00; text-decoration-color: #00ff00; font-weight: bold\">(</span><span style=\"color: #00ff00; text-decoration-color: #00ff00\">Left</span><span style=\"color: #00ff00; text-decoration-color: #00ff00; font-weight: bold\">)</span><span style=\"color: #00ff00; text-decoration-color: #00ff00\"> show that Mixtral achieves a 100%</span>\n",
       "<span style=\"color: #00ff00; text-decoration-color: #00ff00\">retrieval accuracy regardless of the context length or the position of passkey in the sequence. Figure 4 </span><span style=\"color: #00ff00; text-decoration-color: #00ff00; font-weight: bold\">(</span><span style=\"color: #00ff00; text-decoration-color: #00ff00\">Right</span><span style=\"color: #00ff00; text-decoration-color: #00ff00; font-weight: bold\">)</span><span style=\"color: #00ff00; text-decoration-color: #00ff00\"> </span>\n",
       "<span style=\"color: #00ff00; text-decoration-color: #00ff00\">shows that the perplexity of Mixtral on a subset of the proof-pile dataset </span><span style=\"color: #00ff00; text-decoration-color: #00ff00; font-weight: bold\">[</span><span style=\"color: #00ff00; text-decoration-color: #00ff00\">2</span><span style=\"color: #00ff00; text-decoration-color: #00ff00; font-weight: bold\">]</span><span style=\"color: #00ff00; text-decoration-color: #00ff00\"> decreases monotonically as the size </span>\n",
       "<span style=\"color: #00ff00; text-decoration-color: #00ff00\">of the context increases. Passkey Performance ry 0.8 0.6 04 0.2 0.0 OK 4K 8K 12K 16K 20K 24K 28K Seq Len Passkey </span>\n",
       "<span style=\"color: #00ff00; text-decoration-color: #00ff00\">Loc\\n\\nThe chunk discusses Mixtral's performance on multilingual benchmarks and its ability to handle long-range </span>\n",
       "<span style=\"color: #00ff00; text-decoration-color: #00ff00\">context, demonstrating its strong capabilities in these areas.\"</span>, <span style=\"color: #00ff00; text-decoration-color: #00ff00\">'metadata'</span>: <span style=\"font-weight: bold\">{</span><span style=\"color: #00ff00; text-decoration-color: #00ff00\">'title'</span>: <span style=\"color: #00ff00; text-decoration-color: #00ff00\">'Mixtral of Experts'</span>, \n",
       "<span style=\"color: #00ff00; text-decoration-color: #00ff00\">'arxiv_id'</span>: <span style=\"color: #00ff00; text-decoration-color: #00ff00\">'2401.04088'</span>, <span style=\"color: #00ff00; text-decoration-color: #00ff00\">'references'</span>: <span style=\"font-weight: bold\">[</span><span style=\"color: #00ff00; text-decoration-color: #00ff00\">'1905.07830'</span><span style=\"font-weight: bold\">]}}</span>\n",
       "</pre>\n"
      ],
      "text/plain": [
       "Rank \u001b[91m1\u001b[0m \u001b[1m(\u001b[0mscore: \u001b[91m2.11\u001b[0m\u001b[1m)\u001b[0m: \u001b[1m{\u001b[0m\u001b[92m'text'\u001b[0m: \u001b[92m\"Active Params French Arc-c HellaS MMLU German Arc-c HellaS MMLU Spanish Arc-c \u001b[0m\n",
       "\u001b[92mHellaS MMLU Italian Arc-c HellaS MMLU 33B 70B 13B 42.9% 65.4% 49.0% 39.3% 68.1% 49.9% 49.9% 72.5% 64.3% 49.4% 70.9%\u001b[0m\n",
       "\u001b[92m65.1% 58.2% 77.4% 70.9% 54.3% 73.0% 71.5% 55.4% 77.6% 72.5% 52.8% 75.1% 70.9% 41.1% 63.3% 48.7% 47.3% 68.7% 64.2% \u001b[0m\n",
       "\u001b[92m45.7% 69.8% 52.3% 50.5% 74.5% 66.0% Table 4: Comparison of Mixtral with Llama on Multilingual Benchmarks. On ARC \u001b[0m\n",
       "\u001b[92mChallenge, Hellaswag, and MMLU, Mixtral outperforms Llama 2 70B on 4 languages: French, German, Spanish, and \u001b[0m\n",
       "\u001b[92mItalian. # 3.2 Long range performance To assess the capabilities of Mixtral to tackle long context, we evaluate it \u001b[0m\n",
       "\u001b[92mon the passkey retrieval task introduced in \u001b[0m\u001b[1;92m[\u001b[0m\u001b[92m23\u001b[0m\u001b[1;92m]\u001b[0m\u001b[92m, a synthetic task designed to measure the ability of the model to \u001b[0m\n",
       "\u001b[92mretrieve a passkey inserted randomly in a long prompt. Results in Figure 4 \u001b[0m\u001b[1;92m(\u001b[0m\u001b[92mLeft\u001b[0m\u001b[1;92m)\u001b[0m\u001b[92m show that Mixtral achieves a 100%\u001b[0m\n",
       "\u001b[92mretrieval accuracy regardless of the context length or the position of passkey in the sequence. Figure 4 \u001b[0m\u001b[1;92m(\u001b[0m\u001b[92mRight\u001b[0m\u001b[1;92m)\u001b[0m\u001b[92m \u001b[0m\n",
       "\u001b[92mshows that the perplexity of Mixtral on a subset of the proof-pile dataset \u001b[0m\u001b[1;92m[\u001b[0m\u001b[92m2\u001b[0m\u001b[1;92m]\u001b[0m\u001b[92m decreases monotonically as the size \u001b[0m\n",
       "\u001b[92mof the context increases. Passkey Performance ry 0.8 0.6 04 0.2 0.0 OK 4K 8K 12K 16K 20K 24K 28K Seq Len Passkey \u001b[0m\n",
       "\u001b[92mLoc\\n\\nThe chunk discusses Mixtral's performance on multilingual benchmarks and its ability to handle long-range \u001b[0m\n",
       "\u001b[92mcontext, demonstrating its strong capabilities in these areas.\"\u001b[0m, \u001b[92m'metadata'\u001b[0m: \u001b[1m{\u001b[0m\u001b[92m'title'\u001b[0m: \u001b[92m'Mixtral of Experts'\u001b[0m, \n",
       "\u001b[92m'arxiv_id'\u001b[0m: \u001b[92m'2401.04088'\u001b[0m, \u001b[92m'references'\u001b[0m: \u001b[1m[\u001b[0m\u001b[92m'1905.07830'\u001b[0m\u001b[1m]\u001b[0m\u001b[1m}\u001b[0m\u001b[1m}\u001b[0m\n"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       "<pre style=\"white-space:pre;overflow-x:auto;line-height:normal;font-family:Menlo,'DejaVu Sans Mono',consolas,'Courier New',monospace\">Rank <span style=\"color: #ff0000; text-decoration-color: #ff0000\">2</span> <span style=\"font-weight: bold\">(</span>score: <span style=\"color: #ff0000; text-decoration-color: #ff0000\">2.11</span><span style=\"font-weight: bold\">)</span>: <span style=\"font-weight: bold\">{</span><span style=\"color: #00ff00; text-decoration-color: #00ff00\">'text'</span>: <span style=\"color: #00ff00; text-decoration-color: #00ff00\">'expertsâ </span><span style=\"color: #00ff00; text-decoration-color: #00ff00; font-weight: bold\">)</span><span style=\"color: #00ff00; text-decoration-color: #00ff00\"> to process the token and combine their output additively. This technique</span>\n",
       "<span style=\"color: #00ff00; text-decoration-color: #00ff00\">increases the number of parameters of a model while controlling cost and latency, as the model only uses a fraction</span>\n",
       "<span style=\"color: #00ff00; text-decoration-color: #00ff00\">of the total set of parameters per token. Mixtral is pretrained with multilingual data using a context size of 32k </span>\n",
       "<span style=\"color: #00ff00; text-decoration-color: #00ff00\">tokens. It either matches or exceeds the performance of Llama 2 70B and GPT-3.5, over several benchmarks. In </span>\n",
       "<span style=\"color: #00ff00; text-decoration-color: #00ff00\">particular, Mixture of Experts Layer i gating inputs af outputs router expert\\n\\nThis chunk describes the key </span>\n",
       "<span style=\"color: #00ff00; text-decoration-color: #00ff00\">architectural details of the Mixtral model, a sparse mixture-of-experts language model that outperforms larger </span>\n",
       "<span style=\"color: #00ff00; text-decoration-color: #00ff00\">models like Llama 2 70B and GPT-3.5 on various benchmarks.'</span>, <span style=\"color: #00ff00; text-decoration-color: #00ff00\">'metadata'</span>: <span style=\"font-weight: bold\">{</span><span style=\"color: #00ff00; text-decoration-color: #00ff00\">'title'</span>: <span style=\"color: #00ff00; text-decoration-color: #00ff00\">'Mixtral of Experts'</span>, \n",
       "<span style=\"color: #00ff00; text-decoration-color: #00ff00\">'arxiv_id'</span>: <span style=\"color: #00ff00; text-decoration-color: #00ff00\">'2401.04088'</span>, <span style=\"color: #00ff00; text-decoration-color: #00ff00\">'references'</span>: <span style=\"font-weight: bold\">[</span><span style=\"color: #00ff00; text-decoration-color: #00ff00\">'1905.07830'</span><span style=\"font-weight: bold\">]}}</span>\n",
       "</pre>\n"
      ],
      "text/plain": [
       "Rank \u001b[91m2\u001b[0m \u001b[1m(\u001b[0mscore: \u001b[91m2.11\u001b[0m\u001b[1m)\u001b[0m: \u001b[1m{\u001b[0m\u001b[92m'text'\u001b[0m: \u001b[92m'expertsâ \u001b[0m\u001b[1;92m)\u001b[0m\u001b[92m to process the token and combine their output additively. This technique\u001b[0m\n",
       "\u001b[92mincreases the number of parameters of a model while controlling cost and latency, as the model only uses a fraction\u001b[0m\n",
       "\u001b[92mof the total set of parameters per token. Mixtral is pretrained with multilingual data using a context size of 32k \u001b[0m\n",
       "\u001b[92mtokens. It either matches or exceeds the performance of Llama 2 70B and GPT-3.5, over several benchmarks. In \u001b[0m\n",
       "\u001b[92mparticular, Mixture of Experts Layer i gating inputs af outputs router expert\\n\\nThis chunk describes the key \u001b[0m\n",
       "\u001b[92marchitectural details of the Mixtral model, a sparse mixture-of-experts language model that outperforms larger \u001b[0m\n",
       "\u001b[92mmodels like Llama 2 70B and GPT-3.5 on various benchmarks.'\u001b[0m, \u001b[92m'metadata'\u001b[0m: \u001b[1m{\u001b[0m\u001b[92m'title'\u001b[0m: \u001b[92m'Mixtral of Experts'\u001b[0m, \n",
       "\u001b[92m'arxiv_id'\u001b[0m: \u001b[92m'2401.04088'\u001b[0m, \u001b[92m'references'\u001b[0m: \u001b[1m[\u001b[0m\u001b[92m'1905.07830'\u001b[0m\u001b[1m]\u001b[0m\u001b[1m}\u001b[0m\u001b[1m}\u001b[0m\n"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "# Query the corpus\n",
    "query = \"What is context size of Mixtral?\"\n",
    "query_tokens = bm25s.tokenize(query)\n",
    "\n",
    "\n",
    "results, scores = retriever.retrieve(query_tokens, k=2)\n",
    "\n",
    "for i in range(results.shape[1]):\n",
    "    doc, score = results[0, i], scores[0, i]\n",
    "    console.print(f\"Rank {i+1} (score: {score:.2f}): {doc}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": ".venv",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.0"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
