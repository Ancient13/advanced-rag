{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Re-Ranking\n",
    "\n",
    "In the context of RAG (Retrieval-Augmented Generation), reranking of retrieval results is a crucial step that refines the initial set of retrieved documents based on their relevance to the input query. This process involves re-scoring the retrieved documents using a more sophisticated model, such as a cross-encoder, to better capture the semantic similarity between the query and the documents. The reranked list of documents is then used as input for the generation model, ensuring that the most relevant and accurate information is utilized to generate the final output.\n",
    "\n",
    "![Cross Encoder Image](https://raw.githubusercontent.com/UKPLab/sentence-transformers/master/docs/img/CrossEncoder.png)\n",
    "\n",
    "\n",
    "https://www.sbert.net/examples/applications/retrieve_rerank/README.html"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "from rich.console import Console\n",
    "from rich_theme import custom_theme\n",
    "\n",
    "# Create a console with the dark theme\n",
    "console = Console(theme=custom_theme)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "import warnings\n",
    "\n",
    "# Suppress warnings\n",
    "warnings.filterwarnings('ignore')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "84d9bebcc0074fdca54935d74370b51b",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "config.json:   0%|          | 0.00/794 [00:00<?, ?B/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "b463a7e90c834fd590ab9373fd74c589",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "pytorch_model.bin:   0%|          | 0.00/90.9M [00:00<?, ?B/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "93ef3813c3a74c3d920a36136efa2d57",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "tokenizer_config.json:   0%|          | 0.00/316 [00:00<?, ?B/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "ecd787b153594e03a117003b9bcc034f",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "vocab.txt:   0%|          | 0.00/232k [00:00<?, ?B/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "96f5af72766649a39d43460984e9ae24",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "special_tokens_map.json:   0%|          | 0.00/112 [00:00<?, ?B/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "from sentence_transformers import CrossEncoder \n",
    "cross_encoder = CrossEncoder(\"cross-encoder/ms-marco-MiniLM-L-6-v2\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Loading retrival results\n",
    "\n",
    "We will load the retrival results from the previous Hybrid-Search notebook, to avoid repeatition."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "import json\n",
    "retrieved_documents = json.load(open('data/retrieved_docs.json'))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<pre style=\"white-space:pre;overflow-x:auto;line-height:normal;font-family:Menlo,'DejaVu Sans Mono',consolas,'Courier New',monospace\"><span style=\"font-weight: bold\">[</span>\n",
       "    <span style=\"font-weight: bold\">{</span>\n",
       "        <span style=\"color: #00ff00; text-decoration-color: #00ff00\">'id'</span>: <span style=\"color: #ff0000; text-decoration-color: #ff0000\">15</span>,\n",
       "        <span style=\"color: #00ff00; text-decoration-color: #00ff00\">'text'</span>: <span style=\"color: #00ff00; text-decoration-color: #00ff00\">\"3.8 â Mixtral_8x7B 3.5 32 &gt; $3.0 i</span><span style=\"color: #00ff00; text-decoration-color: #00ff00; font-weight: bold\">]</span><span style=\"color: #00ff00; text-decoration-color: #00ff00\"> 228 fos a 2.0 0 5k 10k 15k 20k 25k 30k Context length Passkey </span>\n",
       "<span style=\"color: #00ff00; text-decoration-color: #00ff00\">Performance ry 3.8 â Mixtral_8x7B 3.5 0.8 32 &gt; 0.6 $3.0 i</span><span style=\"color: #00ff00; text-decoration-color: #00ff00; font-weight: bold\">]</span><span style=\"color: #00ff00; text-decoration-color: #00ff00\"> 228 04 fos 0.2 a 2.0 0.0 OK 4K 8K 12K 16K 20K 24K 28K 0 </span>\n",
       "<span style=\"color: #00ff00; text-decoration-color: #00ff00\">5k 10k 15k 20k 25k 30k Seq Len Context length Figure 4: Long range performance of Mixtral. </span><span style=\"color: #00ff00; text-decoration-color: #00ff00; font-weight: bold\">(</span><span style=\"color: #00ff00; text-decoration-color: #00ff00\">Left</span><span style=\"color: #00ff00; text-decoration-color: #00ff00; font-weight: bold\">)</span><span style=\"color: #00ff00; text-decoration-color: #00ff00\"> Mixtral has 100% </span>\n",
       "<span style=\"color: #00ff00; text-decoration-color: #00ff00\">retrieval accuracy of the Passkey task regardless of the location of the passkey and length of the input sequence. </span>\n",
       "<span style=\"color: #00ff00; text-decoration-color: #00ff00; font-weight: bold\">(</span><span style=\"color: #00ff00; text-decoration-color: #00ff00\">Right</span><span style=\"color: #00ff00; text-decoration-color: #00ff00; font-weight: bold\">)</span><span style=\"color: #00ff00; text-decoration-color: #00ff00\"> The perplexity of Mixtral on the proof-pile dataset decreases monotonically as the context length </span>\n",
       "<span style=\"color: #00ff00; text-decoration-color: #00ff00\">increases.\\n\\nThe chunk discusses the long-range performance of the Mixtral model, demonstrating its ability to </span>\n",
       "<span style=\"color: #00ff00; text-decoration-color: #00ff00\">retrieve a passkey regardless of its location in a long input sequence, and showing that the model's perplexity on </span>\n",
       "<span style=\"color: #00ff00; text-decoration-color: #00ff00\">the proof-pile dataset decreases as the context length increases.\"</span>,\n",
       "        <span style=\"color: #00ff00; text-decoration-color: #00ff00\">'dense_score'</span>: <span style=\"color: #ff0000; text-decoration-color: #ff0000\">0.6180975806351034</span>,\n",
       "        <span style=\"color: #00ff00; text-decoration-color: #00ff00\">'sparse_score'</span>: <span style=\"color: #00ff00; text-decoration-color: #00ff00\">'1.4473938'</span>\n",
       "    <span style=\"font-weight: bold\">}</span>,\n",
       "    <span style=\"font-weight: bold\">{</span>\n",
       "        <span style=\"color: #00ff00; text-decoration-color: #00ff00\">'id'</span>: <span style=\"color: #ff0000; text-decoration-color: #ff0000\">4</span>,\n",
       "        <span style=\"color: #00ff00; text-decoration-color: #00ff00\">'text'</span>: <span style=\"color: #00ff00; text-decoration-color: #00ff00\">\"Instruct under the Apache 2.0 license1, free for academic and commercial usage, ensuring broad </span>\n",
       "<span style=\"color: #00ff00; text-decoration-color: #00ff00\">accessibility and potential for diverse applications. To enable the community to run Mixtral with a fully </span>\n",
       "<span style=\"color: #00ff00; text-decoration-color: #00ff00\">open-source stack, we submitted changes to the vLLM project, which integrates Megablocks CUDA kernels for efficient</span>\n",
       "<span style=\"color: #00ff00; text-decoration-color: #00ff00\">inference. Skypilot also allows the deployment of vLLM endpoints on any instance in the cloud. # 2 Architectural </span>\n",
       "<span style=\"color: #00ff00; text-decoration-color: #00ff00\">details Mixtral is based on a transformer architecture </span><span style=\"color: #00ff00; text-decoration-color: #00ff00; font-weight: bold\">[</span><span style=\"color: #00ff00; text-decoration-color: #00ff00\">31</span><span style=\"color: #00ff00; text-decoration-color: #00ff00; font-weight: bold\">]</span><span style=\"color: #00ff00; text-decoration-color: #00ff00\"> and uses the same modifications as described in </span><span style=\"color: #00ff00; text-decoration-color: #00ff00; font-weight: bold\">[</span><span style=\"color: #00ff00; text-decoration-color: #00ff00\">18</span><span style=\"color: #00ff00; text-decoration-color: #00ff00; font-weight: bold\">]</span><span style=\"color: #00ff00; text-decoration-color: #00ff00\">, </span>\n",
       "<span style=\"color: #00ff00; text-decoration-color: #00ff00\">with the notable exceptions that Mix- tral supports a fully dense context length of 32k tokens, and the feed- </span>\n",
       "<span style=\"color: #00ff00; text-decoration-color: #00ff00\">forward blocks are replaced by Mixture-of-Expert layers </span><span style=\"color: #00ff00; text-decoration-color: #00ff00; font-weight: bold\">(</span><span style=\"color: #00ff00; text-decoration-color: #00ff00\">Section 2.1</span><span style=\"color: #00ff00; text-decoration-color: #00ff00; font-weight: bold\">)</span><span style=\"color: #00ff00; text-decoration-color: #00ff00\">. The model architecture parameters are </span>\n",
       "<span style=\"color: #00ff00; text-decoration-color: #00ff00\">summarized in Table 1.\\n\\nThis chunk describes the architectural details of the Mixtral language model, including </span>\n",
       "<span style=\"color: #00ff00; text-decoration-color: #00ff00\">its use of a transformer architecture with a 32k token context length and mixture-of-expert layers. It also </span>\n",
       "<span style=\"color: #00ff00; text-decoration-color: #00ff00\">mentions the model's open-source licensing and deployment options.\"</span>,\n",
       "        <span style=\"color: #00ff00; text-decoration-color: #00ff00\">'dense_score'</span>: <span style=\"color: #ff0000; text-decoration-color: #ff0000\">0.4972174713599332</span>,\n",
       "        <span style=\"color: #00ff00; text-decoration-color: #00ff00\">'sparse_score'</span>: <span style=\"color: #00ff00; text-decoration-color: #00ff00\">'1.1622587'</span>\n",
       "    <span style=\"font-weight: bold\">}</span>,\n",
       "    <span style=\"font-weight: bold\">{</span>\n",
       "        <span style=\"color: #00ff00; text-decoration-color: #00ff00\">'id'</span>: <span style=\"color: #ff0000; text-decoration-color: #ff0000\">2</span>,\n",
       "        <span style=\"color: #00ff00; text-decoration-color: #00ff00\">'text'</span>: <span style=\"color: #00ff00; text-decoration-color: #00ff00\">'expertsâ </span><span style=\"color: #00ff00; text-decoration-color: #00ff00; font-weight: bold\">)</span><span style=\"color: #00ff00; text-decoration-color: #00ff00\"> to process the token and combine their output additively. This technique increases the </span>\n",
       "<span style=\"color: #00ff00; text-decoration-color: #00ff00\">number of parameters of a model while controlling cost and latency, as the model only uses a fraction of the total </span>\n",
       "<span style=\"color: #00ff00; text-decoration-color: #00ff00\">set of parameters per token. Mixtral is pretrained with multilingual data using a context size of 32k tokens. It </span>\n",
       "<span style=\"color: #00ff00; text-decoration-color: #00ff00\">either matches or exceeds the performance of Llama 2 70B and GPT-3.5, over several benchmarks. In particular, </span>\n",
       "<span style=\"color: #00ff00; text-decoration-color: #00ff00\">Mixture of Experts Layer i gating inputs af outputs router expert\\n\\nThis chunk describes the key architectural </span>\n",
       "<span style=\"color: #00ff00; text-decoration-color: #00ff00\">details of the Mixtral model, a sparse mixture-of-experts language model that outperforms larger models like Llama </span>\n",
       "<span style=\"color: #00ff00; text-decoration-color: #00ff00\">2 70B and GPT-3.5 on various benchmarks.'</span>,\n",
       "        <span style=\"color: #00ff00; text-decoration-color: #00ff00\">'dense_score'</span>: <span style=\"color: #ff0000; text-decoration-color: #ff0000\">0.44239135249907124</span>,\n",
       "        <span style=\"color: #00ff00; text-decoration-color: #00ff00\">'sparse_score'</span>: <span style=\"color: #00ff00; text-decoration-color: #00ff00\">'2.1099076'</span>\n",
       "    <span style=\"font-weight: bold\">}</span>,\n",
       "    <span style=\"font-weight: bold\">{</span>\n",
       "        <span style=\"color: #00ff00; text-decoration-color: #00ff00\">'id'</span>: <span style=\"color: #ff0000; text-decoration-color: #ff0000\">6</span>,\n",
       "        <span style=\"color: #00ff00; text-decoration-color: #00ff00\">'text'</span>: <span style=\"color: #00ff00; text-decoration-color: #00ff00\">'Table 1: Model architecture. # j nâ G</span><span style=\"color: #00ff00; text-decoration-color: #00ff00; font-weight: bold\">(</span><span style=\"color: #00ff00; text-decoration-color: #00ff00\">x</span><span style=\"color: #00ff00; text-decoration-color: #00ff00; font-weight: bold\">)</span><span style=\"color: #00ff00; text-decoration-color: #00ff00\">i Â· Ei</span><span style=\"color: #00ff00; text-decoration-color: #00ff00; font-weight: bold\">(</span><span style=\"color: #00ff00; text-decoration-color: #00ff00\">x</span><span style=\"color: #00ff00; text-decoration-color: #00ff00; font-weight: bold\">)</span><span style=\"color: #00ff00; text-decoration-color: #00ff00\">. i=0 Here, G</span><span style=\"color: #00ff00; text-decoration-color: #00ff00; font-weight: bold\">(</span><span style=\"color: #00ff00; text-decoration-color: #00ff00\">x</span><span style=\"color: #00ff00; text-decoration-color: #00ff00; font-weight: bold\">)</span><span style=\"color: #00ff00; text-decoration-color: #00ff00\">i denotes the n-dimensional </span>\n",
       "<span style=\"color: #00ff00; text-decoration-color: #00ff00\">output of the gating network for the i-th expert, and Ei</span><span style=\"color: #00ff00; text-decoration-color: #00ff00; font-weight: bold\">(</span><span style=\"color: #00ff00; text-decoration-color: #00ff00\">x</span><span style=\"color: #00ff00; text-decoration-color: #00ff00; font-weight: bold\">)</span><span style=\"color: #00ff00; text-decoration-color: #00ff00\"> is the output of the i-th expert network. If the gating</span>\n",
       "<span style=\"color: #00ff00; text-decoration-color: #00ff00\">vector is sparse, we can avoid computing the outputs of experts whose gates are zero. There are multiple </span>\n",
       "<span style=\"color: #00ff00; text-decoration-color: #00ff00\">alternative ways of implementing G</span><span style=\"color: #00ff00; text-decoration-color: #00ff00; font-weight: bold\">(</span><span style=\"color: #00ff00; text-decoration-color: #00ff00\">x</span><span style=\"color: #00ff00; text-decoration-color: #00ff00; font-weight: bold\">)</span><span style=\"color: #00ff00; text-decoration-color: #00ff00\"> </span><span style=\"color: #00ff00; text-decoration-color: #00ff00; font-weight: bold\">[</span><span style=\"color: #00ff00; text-decoration-color: #00ff00\">6, 15, 35</span><span style=\"color: #00ff00; text-decoration-color: #00ff00; font-weight: bold\">]</span><span style=\"color: #00ff00; text-decoration-color: #00ff00\">, but a simple and performant one is implemented by taking the </span>\n",
       "<span style=\"color: #00ff00; text-decoration-color: #00ff00\">softmax over the Top-K logits of a linear layer </span><span style=\"color: #00ff00; text-decoration-color: #00ff00; font-weight: bold\">[</span><span style=\"color: #00ff00; text-decoration-color: #00ff00\">28</span><span style=\"color: #00ff00; text-decoration-color: #00ff00; font-weight: bold\">]</span><span style=\"color: #00ff00; text-decoration-color: #00ff00\">.\\n\\nThe chunk describes the architectural details of the </span>\n",
       "<span style=\"color: #00ff00; text-decoration-color: #00ff00\">Mixtral model, specifically the Sparse Mixture of Experts </span><span style=\"color: #00ff00; text-decoration-color: #00ff00; font-weight: bold\">(</span><span style=\"color: #00ff00; text-decoration-color: #00ff00\">SMoE</span><span style=\"color: #00ff00; text-decoration-color: #00ff00; font-weight: bold\">)</span><span style=\"color: #00ff00; text-decoration-color: #00ff00\"> layer that is used in the model.'</span>,\n",
       "        <span style=\"color: #00ff00; text-decoration-color: #00ff00\">'dense_score'</span>: <span style=\"color: #ff0000; text-decoration-color: #ff0000\">0.4404994080056008</span>\n",
       "    <span style=\"font-weight: bold\">}</span>,\n",
       "    <span style=\"font-weight: bold\">{</span>\n",
       "        <span style=\"color: #00ff00; text-decoration-color: #00ff00\">'id'</span>: <span style=\"color: #ff0000; text-decoration-color: #ff0000\">7</span>,\n",
       "        <span style=\"color: #00ff00; text-decoration-color: #00ff00\">'text'</span>: <span style=\"color: #00ff00; text-decoration-color: #00ff00\">'We use G</span><span style=\"color: #00ff00; text-decoration-color: #00ff00; font-weight: bold\">(</span><span style=\"color: #00ff00; text-decoration-color: #00ff00\">x</span><span style=\"color: #00ff00; text-decoration-color: #00ff00; font-weight: bold\">)</span><span style=\"color: #00ff00; text-decoration-color: #00ff00\"> := Softmax</span><span style=\"color: #00ff00; text-decoration-color: #00ff00; font-weight: bold\">(</span><span style=\"color: #00ff00; text-decoration-color: #00ff00\">TopK</span><span style=\"color: #00ff00; text-decoration-color: #00ff00; font-weight: bold\">(</span><span style=\"color: #00ff00; text-decoration-color: #00ff00\">x Â· Wg</span><span style=\"color: #00ff00; text-decoration-color: #00ff00; font-weight: bold\">))</span><span style=\"color: #00ff00; text-decoration-color: #00ff00\">, where </span><span style=\"color: #00ff00; text-decoration-color: #00ff00; font-weight: bold\">(</span><span style=\"color: #00ff00; text-decoration-color: #00ff00\">TopK</span><span style=\"color: #00ff00; text-decoration-color: #00ff00; font-weight: bold\">(</span><span style=\"color: #00ff00; text-decoration-color: #00ff00\">â </span><span style=\"color: #00ff00; text-decoration-color: #00ff00; font-weight: bold\">))</span><span style=\"color: #00ff00; text-decoration-color: #00ff00\">i := â i if â i is among the top-K </span>\n",
       "<span style=\"color: #00ff00; text-decoration-color: #00ff00\">coordinates of logits â â Rn and </span><span style=\"color: #00ff00; text-decoration-color: #00ff00; font-weight: bold\">(</span><span style=\"color: #00ff00; text-decoration-color: #00ff00\">TopK</span><span style=\"color: #00ff00; text-decoration-color: #00ff00; font-weight: bold\">(</span><span style=\"color: #00ff00; text-decoration-color: #00ff00\">â </span><span style=\"color: #00ff00; text-decoration-color: #00ff00; font-weight: bold\">))</span><span style=\"color: #00ff00; text-decoration-color: #00ff00\">i := â â otherwise. The value of K â the number of experts used per </span>\n",
       "<span style=\"color: #00ff00; text-decoration-color: #00ff00\">token â is a hyper-parameter that modu- lates the amount of compute used to process each token. If one increases n </span>\n",
       "<span style=\"color: #00ff00; text-decoration-color: #00ff00\">while keeping K fixed, one # 1https://mistral.ai/news/mixtral-of-experts/\\n\\nThis chunk describes the gating </span>\n",
       "<span style=\"color: #00ff00; text-decoration-color: #00ff00\">mechanism used in the Mixture of Experts </span><span style=\"color: #00ff00; text-decoration-color: #00ff00; font-weight: bold\">(</span><span style=\"color: #00ff00; text-decoration-color: #00ff00\">MoE</span><span style=\"color: #00ff00; text-decoration-color: #00ff00; font-weight: bold\">)</span><span style=\"color: #00ff00; text-decoration-color: #00ff00\"> layer of the Mixtral model. It explains how the router network </span>\n",
       "<span style=\"color: #00ff00; text-decoration-color: #00ff00\">selects the top-K experts to process each token, and how this allows the model to increase its parameter count </span>\n",
       "<span style=\"color: #00ff00; text-decoration-color: #00ff00\">while keeping the computational cost constant.'</span>,\n",
       "        <span style=\"color: #00ff00; text-decoration-color: #00ff00\">'dense_score'</span>: <span style=\"color: #ff0000; text-decoration-color: #ff0000\">0.42453512609457195</span>\n",
       "    <span style=\"font-weight: bold\">}</span>,\n",
       "    <span style=\"font-weight: bold\">{</span>\n",
       "        <span style=\"color: #00ff00; text-decoration-color: #00ff00\">'id'</span>: <span style=\"color: #ff0000; text-decoration-color: #ff0000\">42</span>,\n",
       "        <span style=\"color: #00ff00; text-decoration-color: #00ff00\">'text'</span>: <span style=\"color: #00ff00; text-decoration-color: #00ff00\">'10 </span><span style=\"color: #00ff00; text-decoration-color: #00ff00; font-weight: bold\">[</span><span style=\"color: #00ff00; text-decoration-color: #00ff00\">34</span><span style=\"color: #00ff00; text-decoration-color: #00ff00; font-weight: bold\">]</span><span style=\"color: #00ff00; text-decoration-color: #00ff00\"> Wanjun Zhong, Ruixiang Cui, Yiduo Guo, Yaobo Liang, Shuai Lu, Yanlin Wang, Amin Saied, </span>\n",
       "<span style=\"color: #00ff00; text-decoration-color: #00ff00\">Weizhu Chen, and Nan Duan. Agieval: A human-centric benchmark for evaluating foundation models. arXiv preprint </span>\n",
       "<span style=\"color: #00ff00; text-decoration-color: #00ff00\">arXiv:2304.06364, 2023. </span><span style=\"color: #00ff00; text-decoration-color: #00ff00; font-weight: bold\">[</span><span style=\"color: #00ff00; text-decoration-color: #00ff00\">35</span><span style=\"color: #00ff00; text-decoration-color: #00ff00; font-weight: bold\">]</span><span style=\"color: #00ff00; text-decoration-color: #00ff00\"> Yanqi Zhou, Tao Lei, Hanxiao Liu, Nan Du, Yanping Huang, Vincent Zhao, Andrew M Dai, </span>\n",
       "<span style=\"color: #00ff00; text-decoration-color: #00ff00\">Quoc V Le, James Laudon, et al.\\n\\nThe chunk contains references to related work on evaluating foundation models, </span>\n",
       "<span style=\"color: #00ff00; text-decoration-color: #00ff00\">which is relevant to the overall topic of the document discussing the Mixtral language model.'</span>,\n",
       "        <span style=\"color: #00ff00; text-decoration-color: #00ff00\">'dense_score'</span>: <span style=\"color: #ff0000; text-decoration-color: #ff0000\">0.3848539704676347</span>\n",
       "    <span style=\"font-weight: bold\">}</span>,\n",
       "    <span style=\"font-weight: bold\">{</span>\n",
       "        <span style=\"color: #00ff00; text-decoration-color: #00ff00\">'id'</span>: <span style=\"color: #ff0000; text-decoration-color: #ff0000\">45</span>,\n",
       "        <span style=\"color: #00ff00; text-decoration-color: #00ff00\">'text'</span>: <span style=\"color: #00ff00; text-decoration-color: #00ff00\">'e ArXiv â eâ DM Mathematics â e Github â eâ Gutenberg â eâ PhilPapers â eâ PubMed â e- </span>\n",
       "<span style=\"color: #00ff00; text-decoration-color: #00ff00\">StackExchange â e-â Wikipedia </span><span style=\"color: #00ff00; text-decoration-color: #00ff00; font-weight: bold\">(</span><span style=\"color: #00ff00; text-decoration-color: #00ff00\">en</span><span style=\"color: #00ff00; text-decoration-color: #00ff00; font-weight: bold\">)</span><span style=\"color: #00ff00; text-decoration-color: #00ff00\"> # Abstracts Figure 10: Repeated consecutive assignments per MoE layer. Repeated </span>\n",
       "<span style=\"color: #00ff00; text-decoration-color: #00ff00\">assignments occur a lot more often than they would with uniform assignments </span><span style=\"color: #00ff00; text-decoration-color: #00ff00; font-weight: bold\">(</span><span style=\"color: #00ff00; text-decoration-color: #00ff00\">materialized by the dashed lines</span><span style=\"color: #00ff00; text-decoration-color: #00ff00; font-weight: bold\">)</span><span style=\"color: #00ff00; text-decoration-color: #00ff00\">. </span>\n",
       "<span style=\"color: #00ff00; text-decoration-color: #00ff00\">Patterns are similar across datasets with less repetitions for DM Mathematics. 13\\n\\nThis chunk discusses the </span>\n",
       "<span style=\"color: #00ff00; text-decoration-color: #00ff00\">analysis of expert assignment patterns in the Mixtral model, showing that there is a high degree of temporal </span>\n",
       "<span style=\"color: #00ff00; text-decoration-color: #00ff00\">locality in the expert selection, especially at higher layers of the model. This analysis provides insights into </span>\n",
       "<span style=\"color: #00ff00; text-decoration-color: #00ff00\">the behavior of the sparse mixture-of-experts architecture used in Mixtral.'</span>,\n",
       "        <span style=\"color: #00ff00; text-decoration-color: #00ff00\">'dense_score'</span>: <span style=\"color: #ff0000; text-decoration-color: #ff0000\">0.3803753566702154</span>\n",
       "    <span style=\"font-weight: bold\">}</span>,\n",
       "    <span style=\"font-weight: bold\">{</span>\n",
       "        <span style=\"color: #00ff00; text-decoration-color: #00ff00\">'id'</span>: <span style=\"color: #ff0000; text-decoration-color: #ff0000\">0</span>,\n",
       "        <span style=\"color: #00ff00; text-decoration-color: #00ff00\">'text'</span>: <span style=\"color: #00ff00; text-decoration-color: #00ff00\">'4 2 0 2 n a J 8 </span><span style=\"color: #00ff00; text-decoration-color: #00ff00; font-weight: bold\">]</span><span style=\"color: #00ff00; text-decoration-color: #00ff00\"> G L . s c </span><span style=\"color: #00ff00; text-decoration-color: #00ff00; font-weight: bold\">[</span><span style=\"color: #00ff00; text-decoration-color: #00ff00\"> 1 v 8 8 0 4 0 . 1 0 4 2 : v i X r a # Mixtral of Experts Albert Q. </span>\n",
       "<span style=\"color: #00ff00; text-decoration-color: #00ff00\">Jiang, Alexandre Sablayrolles, Antoine Roux, Arthur Mensch, Blanche Savary, Chris Bamford, Devendra Singh Chaplot, </span>\n",
       "<span style=\"color: #00ff00; text-decoration-color: #00ff00\">Diego de las Casas, Emma Bou Hanna, Florian Bressand, Gianna Lengyel, Guillaume Bour, Guillaume Lample, LÃ©lio </span>\n",
       "<span style=\"color: #00ff00; text-decoration-color: #00ff00\">Renard Lavaud, Lucile Saulnier, Marie-Anne Lachaux, Pierre Stock, Sandeep Subramanian, Sophia Yang, Szymon </span>\n",
       "<span style=\"color: #00ff00; text-decoration-color: #00ff00\">Antoniak, Teven Le Scao, ThÃ©ophile Gervet, Thibaut Lavril, Thomas Wang, TimothÃ©e Lacroix, William El Sayed </span>\n",
       "<span style=\"color: #00ff00; text-decoration-color: #00ff00\">Abstract We introduce Mixtral 8x7B, a Sparse Mixture of Experts </span><span style=\"color: #00ff00; text-decoration-color: #00ff00; font-weight: bold\">(</span><span style=\"color: #00ff00; text-decoration-color: #00ff00\">SMoE</span><span style=\"color: #00ff00; text-decoration-color: #00ff00; font-weight: bold\">)</span><span style=\"color: #00ff00; text-decoration-color: #00ff00\"> language model. Mixtral has the same </span>\n",
       "<span style=\"color: #00ff00; text-decoration-color: #00ff00\">architecture as Mistral 7B, with the difference that each layer is composed of 8 feedforward blocks </span><span style=\"color: #00ff00; text-decoration-color: #00ff00; font-weight: bold\">(</span><span style=\"color: #00ff00; text-decoration-color: #00ff00\">i.e. experts</span><span style=\"color: #00ff00; text-decoration-color: #00ff00; font-weight: bold\">)</span><span style=\"color: #00ff00; text-decoration-color: #00ff00\">.</span>\n",
       "<span style=\"color: #00ff00; text-decoration-color: #00ff00\">For every token, at each layer, a router network selects two experts to process the current state and combine their</span>\n",
       "<span style=\"color: #00ff00; text-decoration-color: #00ff00\">outputs. Even though each token only sees two experts, the selected experts can be different at each timestep. As a</span>\n",
       "<span style=\"color: #00ff00; text-decoration-color: #00ff00\">result, each token has access to 47B parameters, but only uses 13B active parameters during inference. Mixtral was </span>\n",
       "<span style=\"color: #00ff00; text-decoration-color: #00ff00\">trained with a context size of 32k tokens and it outperforms or matches Llama 2 70B and GPT-3.5 across all </span>\n",
       "<span style=\"color: #00ff00; text-decoration-color: #00ff00\">evaluated benchmarks. In particular, Mixtral vastly outperforms Llama 2 70B on mathematics, code generation, and </span>\n",
       "<span style=\"color: #00ff00; text-decoration-color: #00ff00\">multilingual benchmarks. We also provide a model fine- tuned to follow instructions, Mixtral 8x7B â Instruct, that </span>\n",
       "<span style=\"color: #00ff00; text-decoration-color: #00ff00\">surpasses GPT-3.5 Turbo, Claude-2.1, Gemini Pro, and Llama 2 70B â\\n\\nThis chunk introduces Mixtral 8x7B, a sparse </span>\n",
       "<span style=\"color: #00ff00; text-decoration-color: #00ff00\">mixture of experts language model that outperforms Llama 2 70B and GPT-3.5 on various benchmarks. It also describes</span>\n",
       "<span style=\"color: #00ff00; text-decoration-color: #00ff00\">the model architecture and the fine-tuned Mixtral 8x7B - Instruct model.'</span>,\n",
       "        <span style=\"color: #00ff00; text-decoration-color: #00ff00\">'dense_score'</span>: <span style=\"color: #ff0000; text-decoration-color: #ff0000\">0.3677008040909344</span>,\n",
       "        <span style=\"color: #00ff00; text-decoration-color: #00ff00\">'sparse_score'</span>: <span style=\"color: #00ff00; text-decoration-color: #00ff00\">'1.4567579'</span>\n",
       "    <span style=\"font-weight: bold\">}</span>,\n",
       "    <span style=\"font-weight: bold\">{</span>\n",
       "        <span style=\"color: #00ff00; text-decoration-color: #00ff00\">'id'</span>: <span style=\"color: #ff0000; text-decoration-color: #ff0000\">5</span>,\n",
       "        <span style=\"color: #00ff00; text-decoration-color: #00ff00\">'text'</span>: <span style=\"color: #00ff00; text-decoration-color: #00ff00\">'Parameter Value dim n_layers head_dim hidden_dim n_heads n_kv_heads context_len vocab_size </span>\n",
       "<span style=\"color: #00ff00; text-decoration-color: #00ff00\">num_experts top_k_experts # 2.1 Sparse Mixture of Experts We present a brief overview of the Mixture of Experts </span>\n",
       "<span style=\"color: #00ff00; text-decoration-color: #00ff00\">layer </span><span style=\"color: #00ff00; text-decoration-color: #00ff00; font-weight: bold\">(</span><span style=\"color: #00ff00; text-decoration-color: #00ff00\">Figure 1</span><span style=\"color: #00ff00; text-decoration-color: #00ff00; font-weight: bold\">)</span><span style=\"color: #00ff00; text-decoration-color: #00ff00\">. For a more in-depth overview, see </span><span style=\"color: #00ff00; text-decoration-color: #00ff00; font-weight: bold\">[</span><span style=\"color: #00ff00; text-decoration-color: #00ff00\">12</span><span style=\"color: #00ff00; text-decoration-color: #00ff00; font-weight: bold\">]</span><span style=\"color: #00ff00; text-decoration-color: #00ff00\">. The output of the MoE module for a given input x is </span>\n",
       "<span style=\"color: #00ff00; text-decoration-color: #00ff00\">determined by the weighted sum of the outputs of the expert networks, where the weights are given by the gating </span>\n",
       "<span style=\"color: #00ff00; text-decoration-color: #00ff00\">networkâ s output. i.e. given n expert networks </span><span style=\"color: #00ff00; text-decoration-color: #00ff00; font-weight: bold\">{</span><span style=\"color: #00ff00; text-decoration-color: #00ff00\">E0, Ei, ..., Enâ 1</span><span style=\"color: #00ff00; text-decoration-color: #00ff00; font-weight: bold\">}</span><span style=\"color: #00ff00; text-decoration-color: #00ff00\">, the output of the expert layer is given </span>\n",
       "<span style=\"color: #00ff00; text-decoration-color: #00ff00\">by:\\n\\nThis chunk describes the architectural details of the Mixtral model, specifically the Sparse Mixture of </span>\n",
       "<span style=\"color: #00ff00; text-decoration-color: #00ff00\">Experts layer that is a key component of the model.'</span>,\n",
       "        <span style=\"color: #00ff00; text-decoration-color: #00ff00\">'dense_score'</span>: <span style=\"color: #ff0000; text-decoration-color: #ff0000\">0.36711093531341693</span>\n",
       "    <span style=\"font-weight: bold\">}</span>,\n",
       "    <span style=\"font-weight: bold\">{</span>\n",
       "        <span style=\"color: #00ff00; text-decoration-color: #00ff00\">'id'</span>: <span style=\"color: #ff0000; text-decoration-color: #ff0000\">11</span>,\n",
       "        <span style=\"color: #00ff00; text-decoration-color: #00ff00\">'text'</span>: <span style=\"color: #00ff00; text-decoration-color: #00ff00\">'Mistral 78 % 2681 Mistral 78 3 3 s0 5 = A % 66 50 g 4 45 64 78 138 348708 78 138 348708 78 138 348</span>\n",
       "<span style=\"color: #00ff00; text-decoration-color: #00ff00\">70B S66 Mixtral 8x7B 50 Mixtral 8x7B 5 = 564 340 g al Mistral 78 ee Mistral 78 3 5 Â§ 30 5 eo â = Mistral Â° 20 â e</span>\n",
       "<span style=\"color: #00ff00; text-decoration-color: #00ff00\">LlaMA2 78 </span><span style=\"color: #00ff00; text-decoration-color: #00ff00; font-weight: bold\">(</span><span style=\"color: #00ff00; text-decoration-color: #00ff00\">138 348 70B 7B </span><span style=\"color: #00ff00; text-decoration-color: #00ff00; font-weight: bold\">(</span><span style=\"color: #00ff00; text-decoration-color: #00ff00\">138 348 708 7B Â«13B 34B 708 Active Params Active Params Active Params Figure 3: Results</span>\n",
       "<span style=\"color: #00ff00; text-decoration-color: #00ff00\">on MMLU, commonsense reasoning, world knowledge and reading comprehension, math and code for Mistral </span><span style=\"color: #00ff00; text-decoration-color: #00ff00; font-weight: bold\">(</span><span style=\"color: #00ff00; text-decoration-color: #00ff00\">7B/8x7B</span><span style=\"color: #00ff00; text-decoration-color: #00ff00; font-weight: bold\">)</span><span style=\"color: #00ff00; text-decoration-color: #00ff00\"> vs </span>\n",
       "<span style=\"color: #00ff00; text-decoration-color: #00ff00\">Llama 2 </span><span style=\"color: #00ff00; text-decoration-color: #00ff00; font-weight: bold\">(</span><span style=\"color: #00ff00; text-decoration-color: #00ff00\">7B/13B/70B</span><span style=\"color: #00ff00; text-decoration-color: #00ff00; font-weight: bold\">)</span><span style=\"color: #00ff00; text-decoration-color: #00ff00\">. Mixtral largely outperforms Llama 2 70B on all benchmarks, except on reading comprehension </span>\n",
       "<span style=\"color: #00ff00; text-decoration-color: #00ff00\">benchmarks while using 5x lower active parameters. It is also vastly superior to Llama 2 70B on code and math. </span>\n",
       "<span style=\"color: #00ff00; text-decoration-color: #00ff00\">Detailed results for Mixtral, Mistral 7B and Llama 2 7B/13B/70B and Llama 1 34B2 are reported in Table 2. Figure 2 </span>\n",
       "<span style=\"color: #00ff00; text-decoration-color: #00ff00\">compares the performance of Mixtral with the Llama models in different categories. Mixtral surpasses Llama 2 70B </span>\n",
       "<span style=\"color: #00ff00; text-decoration-color: #00ff00\">across most metrics. In particular, Mixtral displays a superior performance in code and mathematics </span>\n",
       "<span style=\"color: #00ff00; text-decoration-color: #00ff00\">benchmarks.\\n\\nThis chunk presents a comparison of the performance of the Mixtral 8x7B and Mistral 7B models </span>\n",
       "<span style=\"color: #00ff00; text-decoration-color: #00ff00\">against the Llama 2 family of models across various benchmarks, including commonsense reasoning, world knowledge, </span>\n",
       "<span style=\"color: #00ff00; text-decoration-color: #00ff00\">reading comprehension, math, and code generation. It highlights that Mixtral outperforms Llama 2 70B on most </span>\n",
       "<span style=\"color: #00ff00; text-decoration-color: #00ff00\">metrics while using significantly fewer active parameters.'</span>,\n",
       "        <span style=\"color: #00ff00; text-decoration-color: #00ff00\">'dense_score'</span>: <span style=\"color: #ff0000; text-decoration-color: #ff0000\">0.35405535832805446</span>\n",
       "    <span style=\"font-weight: bold\">}</span>\n",
       "<span style=\"font-weight: bold\">]</span>\n",
       "</pre>\n"
      ],
      "text/plain": [
       "\u001b[1m[\u001b[0m\n",
       "    \u001b[1m{\u001b[0m\n",
       "        \u001b[92m'id'\u001b[0m: \u001b[91m15\u001b[0m,\n",
       "        \u001b[92m'text'\u001b[0m: \u001b[92m\"3.8 â Mixtral_8x7B 3.5 32 > $3.0 i\u001b[0m\u001b[1;92m]\u001b[0m\u001b[92m 228 fos a 2.0 0 5k 10k 15k 20k 25k 30k Context length Passkey \u001b[0m\n",
       "\u001b[92mPerformance ry 3.8 â Mixtral_8x7B 3.5 0.8 32 > 0.6 $3.0 i\u001b[0m\u001b[1;92m]\u001b[0m\u001b[92m 228 04 fos 0.2 a 2.0 0.0 OK 4K 8K 12K 16K 20K 24K 28K 0 \u001b[0m\n",
       "\u001b[92m5k 10k 15k 20k 25k 30k Seq Len Context length Figure 4: Long range performance of Mixtral. \u001b[0m\u001b[1;92m(\u001b[0m\u001b[92mLeft\u001b[0m\u001b[1;92m)\u001b[0m\u001b[92m Mixtral has 100% \u001b[0m\n",
       "\u001b[92mretrieval accuracy of the Passkey task regardless of the location of the passkey and length of the input sequence. \u001b[0m\n",
       "\u001b[1;92m(\u001b[0m\u001b[92mRight\u001b[0m\u001b[1;92m)\u001b[0m\u001b[92m The perplexity of Mixtral on the proof-pile dataset decreases monotonically as the context length \u001b[0m\n",
       "\u001b[92mincreases.\\n\\nThe chunk discusses the long-range performance of the Mixtral model, demonstrating its ability to \u001b[0m\n",
       "\u001b[92mretrieve a passkey regardless of its location in a long input sequence, and showing that the model's perplexity on \u001b[0m\n",
       "\u001b[92mthe proof-pile dataset decreases as the context length increases.\"\u001b[0m,\n",
       "        \u001b[92m'dense_score'\u001b[0m: \u001b[91m0.6180975806351034\u001b[0m,\n",
       "        \u001b[92m'sparse_score'\u001b[0m: \u001b[92m'1.4473938'\u001b[0m\n",
       "    \u001b[1m}\u001b[0m,\n",
       "    \u001b[1m{\u001b[0m\n",
       "        \u001b[92m'id'\u001b[0m: \u001b[91m4\u001b[0m,\n",
       "        \u001b[92m'text'\u001b[0m: \u001b[92m\"Instruct under the Apache 2.0 license1, free for academic and commercial usage, ensuring broad \u001b[0m\n",
       "\u001b[92maccessibility and potential for diverse applications. To enable the community to run Mixtral with a fully \u001b[0m\n",
       "\u001b[92mopen-source stack, we submitted changes to the vLLM project, which integrates Megablocks CUDA kernels for efficient\u001b[0m\n",
       "\u001b[92minference. Skypilot also allows the deployment of vLLM endpoints on any instance in the cloud. # 2 Architectural \u001b[0m\n",
       "\u001b[92mdetails Mixtral is based on a transformer architecture \u001b[0m\u001b[1;92m[\u001b[0m\u001b[92m31\u001b[0m\u001b[1;92m]\u001b[0m\u001b[92m and uses the same modifications as described in \u001b[0m\u001b[1;92m[\u001b[0m\u001b[92m18\u001b[0m\u001b[1;92m]\u001b[0m\u001b[92m, \u001b[0m\n",
       "\u001b[92mwith the notable exceptions that Mix- tral supports a fully dense context length of 32k tokens, and the feed- \u001b[0m\n",
       "\u001b[92mforward blocks are replaced by Mixture-of-Expert layers \u001b[0m\u001b[1;92m(\u001b[0m\u001b[92mSection 2.1\u001b[0m\u001b[1;92m)\u001b[0m\u001b[92m. The model architecture parameters are \u001b[0m\n",
       "\u001b[92msummarized in Table 1.\\n\\nThis chunk describes the architectural details of the Mixtral language model, including \u001b[0m\n",
       "\u001b[92mits use of a transformer architecture with a 32k token context length and mixture-of-expert layers. It also \u001b[0m\n",
       "\u001b[92mmentions the model's open-source licensing and deployment options.\"\u001b[0m,\n",
       "        \u001b[92m'dense_score'\u001b[0m: \u001b[91m0.4972174713599332\u001b[0m,\n",
       "        \u001b[92m'sparse_score'\u001b[0m: \u001b[92m'1.1622587'\u001b[0m\n",
       "    \u001b[1m}\u001b[0m,\n",
       "    \u001b[1m{\u001b[0m\n",
       "        \u001b[92m'id'\u001b[0m: \u001b[91m2\u001b[0m,\n",
       "        \u001b[92m'text'\u001b[0m: \u001b[92m'expertsâ \u001b[0m\u001b[1;92m)\u001b[0m\u001b[92m to process the token and combine their output additively. This technique increases the \u001b[0m\n",
       "\u001b[92mnumber of parameters of a model while controlling cost and latency, as the model only uses a fraction of the total \u001b[0m\n",
       "\u001b[92mset of parameters per token. Mixtral is pretrained with multilingual data using a context size of 32k tokens. It \u001b[0m\n",
       "\u001b[92meither matches or exceeds the performance of Llama 2 70B and GPT-3.5, over several benchmarks. In particular, \u001b[0m\n",
       "\u001b[92mMixture of Experts Layer i gating inputs af outputs router expert\\n\\nThis chunk describes the key architectural \u001b[0m\n",
       "\u001b[92mdetails of the Mixtral model, a sparse mixture-of-experts language model that outperforms larger models like Llama \u001b[0m\n",
       "\u001b[92m2 70B and GPT-3.5 on various benchmarks.'\u001b[0m,\n",
       "        \u001b[92m'dense_score'\u001b[0m: \u001b[91m0.44239135249907124\u001b[0m,\n",
       "        \u001b[92m'sparse_score'\u001b[0m: \u001b[92m'2.1099076'\u001b[0m\n",
       "    \u001b[1m}\u001b[0m,\n",
       "    \u001b[1m{\u001b[0m\n",
       "        \u001b[92m'id'\u001b[0m: \u001b[91m6\u001b[0m,\n",
       "        \u001b[92m'text'\u001b[0m: \u001b[92m'Table 1: Model architecture. # j nâ G\u001b[0m\u001b[1;92m(\u001b[0m\u001b[92mx\u001b[0m\u001b[1;92m)\u001b[0m\u001b[92mi Â· Ei\u001b[0m\u001b[1;92m(\u001b[0m\u001b[92mx\u001b[0m\u001b[1;92m)\u001b[0m\u001b[92m. \u001b[0m\u001b[92mi\u001b[0m\u001b[92m=\u001b[0m\u001b[92m0\u001b[0m\u001b[92m Here, G\u001b[0m\u001b[1;92m(\u001b[0m\u001b[92mx\u001b[0m\u001b[1;92m)\u001b[0m\u001b[92mi denotes the n-dimensional \u001b[0m\n",
       "\u001b[92moutput of the gating network for the i-th expert, and Ei\u001b[0m\u001b[1;92m(\u001b[0m\u001b[92mx\u001b[0m\u001b[1;92m)\u001b[0m\u001b[92m is the output of the i-th expert network. If the gating\u001b[0m\n",
       "\u001b[92mvector is sparse, we can avoid computing the outputs of experts whose gates are zero. There are multiple \u001b[0m\n",
       "\u001b[92malternative ways of implementing G\u001b[0m\u001b[1;92m(\u001b[0m\u001b[92mx\u001b[0m\u001b[1;92m)\u001b[0m\u001b[92m \u001b[0m\u001b[1;92m[\u001b[0m\u001b[92m6, 15, 35\u001b[0m\u001b[1;92m]\u001b[0m\u001b[92m, but a simple and performant one is implemented by taking the \u001b[0m\n",
       "\u001b[92msoftmax over the Top-K logits of a linear layer \u001b[0m\u001b[1;92m[\u001b[0m\u001b[92m28\u001b[0m\u001b[1;92m]\u001b[0m\u001b[92m.\\n\\nThe chunk describes the architectural details of the \u001b[0m\n",
       "\u001b[92mMixtral model, specifically the Sparse Mixture of Experts \u001b[0m\u001b[1;92m(\u001b[0m\u001b[92mSMoE\u001b[0m\u001b[1;92m)\u001b[0m\u001b[92m layer that is used in the model.'\u001b[0m,\n",
       "        \u001b[92m'dense_score'\u001b[0m: \u001b[91m0.4404994080056008\u001b[0m\n",
       "    \u001b[1m}\u001b[0m,\n",
       "    \u001b[1m{\u001b[0m\n",
       "        \u001b[92m'id'\u001b[0m: \u001b[91m7\u001b[0m,\n",
       "        \u001b[92m'text'\u001b[0m: \u001b[92m'We use G\u001b[0m\u001b[1;92m(\u001b[0m\u001b[92mx\u001b[0m\u001b[1;92m)\u001b[0m\u001b[92m := Softmax\u001b[0m\u001b[1;92m(\u001b[0m\u001b[92mTopK\u001b[0m\u001b[1;92m(\u001b[0m\u001b[92mx Â· Wg\u001b[0m\u001b[1;92m)\u001b[0m\u001b[1;92m)\u001b[0m\u001b[92m, where \u001b[0m\u001b[1;92m(\u001b[0m\u001b[92mTopK\u001b[0m\u001b[1;92m(\u001b[0m\u001b[92mâ \u001b[0m\u001b[1;92m)\u001b[0m\u001b[1;92m)\u001b[0m\u001b[92mi := â i if â i is among the top-K \u001b[0m\n",
       "\u001b[92mcoordinates of logits â â Rn and \u001b[0m\u001b[1;92m(\u001b[0m\u001b[92mTopK\u001b[0m\u001b[1;92m(\u001b[0m\u001b[92mâ \u001b[0m\u001b[1;92m)\u001b[0m\u001b[1;92m)\u001b[0m\u001b[92mi := â â otherwise. The value of K â the number of experts used per \u001b[0m\n",
       "\u001b[92mtoken â is a hyper-parameter that modu- lates the amount of compute used to process each token. If one increases n \u001b[0m\n",
       "\u001b[92mwhile keeping K fixed, one # 1https://mistral.ai/news/mixtral-of-experts/\\n\\nThis chunk describes the gating \u001b[0m\n",
       "\u001b[92mmechanism used in the Mixture of Experts \u001b[0m\u001b[1;92m(\u001b[0m\u001b[92mMoE\u001b[0m\u001b[1;92m)\u001b[0m\u001b[92m layer of the Mixtral model. It explains how the router network \u001b[0m\n",
       "\u001b[92mselects the top-K experts to process each token, and how this allows the model to increase its parameter count \u001b[0m\n",
       "\u001b[92mwhile keeping the computational cost constant.'\u001b[0m,\n",
       "        \u001b[92m'dense_score'\u001b[0m: \u001b[91m0.42453512609457195\u001b[0m\n",
       "    \u001b[1m}\u001b[0m,\n",
       "    \u001b[1m{\u001b[0m\n",
       "        \u001b[92m'id'\u001b[0m: \u001b[91m42\u001b[0m,\n",
       "        \u001b[92m'text'\u001b[0m: \u001b[92m'10 \u001b[0m\u001b[1;92m[\u001b[0m\u001b[92m34\u001b[0m\u001b[1;92m]\u001b[0m\u001b[92m Wanjun Zhong, Ruixiang Cui, Yiduo Guo, Yaobo Liang, Shuai Lu, Yanlin Wang, Amin Saied, \u001b[0m\n",
       "\u001b[92mWeizhu Chen, and Nan Duan. Agieval: A human-centric benchmark for evaluating foundation models. arXiv preprint \u001b[0m\n",
       "\u001b[92marXiv:2304.06364, 2023. \u001b[0m\u001b[1;92m[\u001b[0m\u001b[92m35\u001b[0m\u001b[1;92m]\u001b[0m\u001b[92m Yanqi Zhou, Tao Lei, Hanxiao Liu, Nan Du, Yanping Huang, Vincent Zhao, Andrew M Dai, \u001b[0m\n",
       "\u001b[92mQuoc V Le, James Laudon, et al.\\n\\nThe chunk contains references to related work on evaluating foundation models, \u001b[0m\n",
       "\u001b[92mwhich is relevant to the overall topic of the document discussing the Mixtral language model.'\u001b[0m,\n",
       "        \u001b[92m'dense_score'\u001b[0m: \u001b[91m0.3848539704676347\u001b[0m\n",
       "    \u001b[1m}\u001b[0m,\n",
       "    \u001b[1m{\u001b[0m\n",
       "        \u001b[92m'id'\u001b[0m: \u001b[91m45\u001b[0m,\n",
       "        \u001b[92m'text'\u001b[0m: \u001b[92m'e ArXiv â eâ DM Mathematics â e Github â eâ Gutenberg â eâ PhilPapers â eâ PubMed â e- \u001b[0m\n",
       "\u001b[92mStackExchange â e-â Wikipedia \u001b[0m\u001b[1;92m(\u001b[0m\u001b[92men\u001b[0m\u001b[1;92m)\u001b[0m\u001b[92m # Abstracts Figure 10: Repeated consecutive assignments per MoE layer. Repeated \u001b[0m\n",
       "\u001b[92massignments occur a lot more often than they would with uniform assignments \u001b[0m\u001b[1;92m(\u001b[0m\u001b[92mmaterialized by the dashed lines\u001b[0m\u001b[1;92m)\u001b[0m\u001b[92m. \u001b[0m\n",
       "\u001b[92mPatterns are similar across datasets with less repetitions for DM Mathematics. 13\\n\\nThis chunk discusses the \u001b[0m\n",
       "\u001b[92manalysis of expert assignment patterns in the Mixtral model, showing that there is a high degree of temporal \u001b[0m\n",
       "\u001b[92mlocality in the expert selection, especially at higher layers of the model. This analysis provides insights into \u001b[0m\n",
       "\u001b[92mthe behavior of the sparse mixture-of-experts architecture used in Mixtral.'\u001b[0m,\n",
       "        \u001b[92m'dense_score'\u001b[0m: \u001b[91m0.3803753566702154\u001b[0m\n",
       "    \u001b[1m}\u001b[0m,\n",
       "    \u001b[1m{\u001b[0m\n",
       "        \u001b[92m'id'\u001b[0m: \u001b[91m0\u001b[0m,\n",
       "        \u001b[92m'text'\u001b[0m: \u001b[92m'4 2 0 2 n a J 8 \u001b[0m\u001b[1;92m]\u001b[0m\u001b[92m G L . s c \u001b[0m\u001b[1;92m[\u001b[0m\u001b[92m 1 v 8 8 0 4 0 . 1 0 4 2 : v i X r a # Mixtral of Experts Albert Q. \u001b[0m\n",
       "\u001b[92mJiang, Alexandre Sablayrolles, Antoine Roux, Arthur Mensch, Blanche Savary, Chris Bamford, Devendra Singh Chaplot, \u001b[0m\n",
       "\u001b[92mDiego de las Casas, Emma Bou Hanna, Florian Bressand, Gianna Lengyel, Guillaume Bour, Guillaume Lample, LÃ©lio \u001b[0m\n",
       "\u001b[92mRenard Lavaud, Lucile Saulnier, Marie-Anne Lachaux, Pierre Stock, Sandeep Subramanian, Sophia Yang, Szymon \u001b[0m\n",
       "\u001b[92mAntoniak, Teven Le Scao, ThÃ©ophile Gervet, Thibaut Lavril, Thomas Wang, TimothÃ©e Lacroix, William El Sayed \u001b[0m\n",
       "\u001b[92mAbstract We introduce Mixtral 8x7B, a Sparse Mixture of Experts \u001b[0m\u001b[1;92m(\u001b[0m\u001b[92mSMoE\u001b[0m\u001b[1;92m)\u001b[0m\u001b[92m language model. Mixtral has the same \u001b[0m\n",
       "\u001b[92marchitecture as Mistral 7B, with the difference that each layer is composed of 8 feedforward blocks \u001b[0m\u001b[1;92m(\u001b[0m\u001b[92mi.e. experts\u001b[0m\u001b[1;92m)\u001b[0m\u001b[92m.\u001b[0m\n",
       "\u001b[92mFor every token, at each layer, a router network selects two experts to process the current state and combine their\u001b[0m\n",
       "\u001b[92moutputs. Even though each token only sees two experts, the selected experts can be different at each timestep. As a\u001b[0m\n",
       "\u001b[92mresult, each token has access to 47B parameters, but only uses 13B active parameters during inference. Mixtral was \u001b[0m\n",
       "\u001b[92mtrained with a context size of 32k tokens and it outperforms or matches Llama 2 70B and GPT-3.5 across all \u001b[0m\n",
       "\u001b[92mevaluated benchmarks. In particular, Mixtral vastly outperforms Llama 2 70B on mathematics, code generation, and \u001b[0m\n",
       "\u001b[92mmultilingual benchmarks. We also provide a model fine- tuned to follow instructions, Mixtral 8x7B â Instruct, that \u001b[0m\n",
       "\u001b[92msurpasses GPT-3.5 Turbo, Claude-2.1, Gemini Pro, and Llama 2 70B â\\n\\nThis chunk introduces Mixtral 8x7B, a sparse \u001b[0m\n",
       "\u001b[92mmixture of experts language model that outperforms Llama 2 70B and GPT-3.5 on various benchmarks. It also describes\u001b[0m\n",
       "\u001b[92mthe model architecture and the fine-tuned Mixtral 8x7B - Instruct model.'\u001b[0m,\n",
       "        \u001b[92m'dense_score'\u001b[0m: \u001b[91m0.3677008040909344\u001b[0m,\n",
       "        \u001b[92m'sparse_score'\u001b[0m: \u001b[92m'1.4567579'\u001b[0m\n",
       "    \u001b[1m}\u001b[0m,\n",
       "    \u001b[1m{\u001b[0m\n",
       "        \u001b[92m'id'\u001b[0m: \u001b[91m5\u001b[0m,\n",
       "        \u001b[92m'text'\u001b[0m: \u001b[92m'Parameter Value dim n_layers head_dim hidden_dim n_heads n_kv_heads context_len vocab_size \u001b[0m\n",
       "\u001b[92mnum_experts top_k_experts # 2.1 Sparse Mixture of Experts We present a brief overview of the Mixture of Experts \u001b[0m\n",
       "\u001b[92mlayer \u001b[0m\u001b[1;92m(\u001b[0m\u001b[92mFigure 1\u001b[0m\u001b[1;92m)\u001b[0m\u001b[92m. For a more in-depth overview, see \u001b[0m\u001b[1;92m[\u001b[0m\u001b[92m12\u001b[0m\u001b[1;92m]\u001b[0m\u001b[92m. The output of the MoE module for a given input x is \u001b[0m\n",
       "\u001b[92mdetermined by the weighted sum of the outputs of the expert networks, where the weights are given by the gating \u001b[0m\n",
       "\u001b[92mnetworkâ s output. i.e. given n expert networks \u001b[0m\u001b[1;92m{\u001b[0m\u001b[92mE0, Ei, ..., Enâ 1\u001b[0m\u001b[1;92m}\u001b[0m\u001b[92m, the output of the expert layer is given \u001b[0m\n",
       "\u001b[92mby:\\n\\nThis chunk describes the architectural details of the Mixtral model, specifically the Sparse Mixture of \u001b[0m\n",
       "\u001b[92mExperts layer that is a key component of the model.'\u001b[0m,\n",
       "        \u001b[92m'dense_score'\u001b[0m: \u001b[91m0.36711093531341693\u001b[0m\n",
       "    \u001b[1m}\u001b[0m,\n",
       "    \u001b[1m{\u001b[0m\n",
       "        \u001b[92m'id'\u001b[0m: \u001b[91m11\u001b[0m,\n",
       "        \u001b[92m'text'\u001b[0m: \u001b[92m'Mistral 78 % 2681 Mistral 78 3 3 s0 5 = A % 66 50 g 4 45 64 78 138 348708 78 138 348708 78 138 348\u001b[0m\n",
       "\u001b[92m70B S66 Mixtral 8x7B 50 Mixtral 8x7B 5 = 564 340 g al Mistral 78 ee Mistral 78 3 5 Â§ 30 5 eo â = Mistral Â° 20 â e\u001b[0m\n",
       "\u001b[92mLlaMA2 78 \u001b[0m\u001b[1;92m(\u001b[0m\u001b[92m138 348 70B 7B \u001b[0m\u001b[1;92m(\u001b[0m\u001b[92m138 348 708 7B Â«13B 34B 708 Active Params Active Params Active Params Figure 3: Results\u001b[0m\n",
       "\u001b[92mon MMLU, commonsense reasoning, world knowledge and reading comprehension, math and code for Mistral \u001b[0m\u001b[1;92m(\u001b[0m\u001b[92m7B/8x7B\u001b[0m\u001b[1;92m)\u001b[0m\u001b[92m vs \u001b[0m\n",
       "\u001b[92mLlama 2 \u001b[0m\u001b[1;92m(\u001b[0m\u001b[92m7B/13B/70B\u001b[0m\u001b[1;92m)\u001b[0m\u001b[92m. Mixtral largely outperforms Llama 2 70B on all benchmarks, except on reading comprehension \u001b[0m\n",
       "\u001b[92mbenchmarks while using 5x lower active parameters. It is also vastly superior to Llama 2 70B on code and math. \u001b[0m\n",
       "\u001b[92mDetailed results for Mixtral, Mistral 7B and Llama 2 7B/13B/70B and Llama 1 34B2 are reported in Table 2. Figure 2 \u001b[0m\n",
       "\u001b[92mcompares the performance of Mixtral with the Llama models in different categories. Mixtral surpasses Llama 2 70B \u001b[0m\n",
       "\u001b[92macross most metrics. In particular, Mixtral displays a superior performance in code and mathematics \u001b[0m\n",
       "\u001b[92mbenchmarks.\\n\\nThis chunk presents a comparison of the performance of the Mixtral 8x7B and Mistral 7B models \u001b[0m\n",
       "\u001b[92magainst the Llama 2 family of models across various benchmarks, including commonsense reasoning, world knowledge, \u001b[0m\n",
       "\u001b[92mreading comprehension, math, and code generation. It highlights that Mixtral outperforms Llama 2 70B on most \u001b[0m\n",
       "\u001b[92mmetrics while using significantly fewer active parameters.'\u001b[0m,\n",
       "        \u001b[92m'dense_score'\u001b[0m: \u001b[91m0.35405535832805446\u001b[0m\n",
       "    \u001b[1m}\u001b[0m\n",
       "\u001b[1m]\u001b[0m\n"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "console.print(retrieved_documents)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {},
   "outputs": [],
   "source": [
    "# This is the query that we used for the retrieval of the above documents\n",
    "query = \"What is context size of Mixtral?\"\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Calculating the re-ranking scores"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<pre style=\"white-space:pre;overflow-x:auto;line-height:normal;font-family:Menlo,'DejaVu Sans Mono',consolas,'Courier New',monospace\"><span style=\"font-weight: bold\">[</span> <span style=\"color: #ff0000; text-decoration-color: #ff0000\">5.065694</span>   <span style=\"color: #ff0000; text-decoration-color: #ff0000\">3.368832</span>   <span style=\"color: #ff0000; text-decoration-color: #ff0000\">7.1048393</span> <span style=\"color: #ff0000; text-decoration-color: #ff0000\">-4.116105</span>  <span style=\"color: #ff0000; text-decoration-color: #ff0000\">-4.3754997</span> <span style=\"color: #ff0000; text-decoration-color: #ff0000\">-5.2610755</span>\n",
       " <span style=\"color: #ff0000; text-decoration-color: #ff0000\">-3.7225761</span>  <span style=\"color: #ff0000; text-decoration-color: #ff0000\">3.1854553</span>  <span style=\"color: #ff0000; text-decoration-color: #ff0000\">1.7966702</span> <span style=\"color: #ff0000; text-decoration-color: #ff0000\">-2.5144258</span><span style=\"font-weight: bold\">]</span>\n",
       "</pre>\n"
      ],
      "text/plain": [
       "\u001b[1m[\u001b[0m \u001b[91m5.065694\u001b[0m   \u001b[91m3.368832\u001b[0m   \u001b[91m7.1048393\u001b[0m \u001b[91m-4.116105\u001b[0m  \u001b[91m-4.3754997\u001b[0m \u001b[91m-5.2610755\u001b[0m\n",
       " \u001b[91m-3.7225761\u001b[0m  \u001b[91m3.1854553\u001b[0m  \u001b[91m1.7966702\u001b[0m \u001b[91m-2.5144258\u001b[0m\u001b[1m]\u001b[0m\n"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "pairs = [[query, doc['text']] for doc in retrieved_documents] \n",
    "scores = cross_encoder.predict(pairs) \n",
    "\n",
    "console.print(scores)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Selecting top 3 reranked documents"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Top 3 Documents:\n",
      "expertsâ ) to process the token and combine their output additively. This technique increases the number of parameters of a model while controlling cost and latency, as the model only uses a fraction of the total set of parameters per token. Mixtral is pretrained with multilingual data using a context size of 32k tokens. It either matches or exceeds the performance of Llama 2 70B and GPT-3.5, over several benchmarks. In particular, Mixture of Experts Layer i gating inputs af outputs router expert\n",
      "\n",
      "This chunk describes the key architectural details of the Mixtral model, a sparse mixture-of-experts language model that outperforms larger models like Llama 2 70B and GPT-3.5 on various benchmarks.\n",
      "3.8 â Mixtral_8x7B 3.5 32 > $3.0 i] 228 fos a 2.0 0 5k 10k 15k 20k 25k 30k Context length Passkey Performance ry 3.8 â Mixtral_8x7B 3.5 0.8 32 > 0.6 $3.0 i] 228 04 fos 0.2 a 2.0 0.0 OK 4K 8K 12K 16K 20K 24K 28K 0 5k 10k 15k 20k 25k 30k Seq Len Context length Figure 4: Long range performance of Mixtral. (Left) Mixtral has 100% retrieval accuracy of the Passkey task regardless of the location of the passkey and length of the input sequence. (Right) The perplexity of Mixtral on the proof-pile dataset decreases monotonically as the context length increases.\n",
      "\n",
      "The chunk discusses the long-range performance of the Mixtral model, demonstrating its ability to retrieve a passkey regardless of its location in a long input sequence, and showing that the model's perplexity on the proof-pile dataset decreases as the context length increases.\n",
      "Instruct under the Apache 2.0 license1, free for academic and commercial usage, ensuring broad accessibility and potential for diverse applications. To enable the community to run Mixtral with a fully open-source stack, we submitted changes to the vLLM project, which integrates Megablocks CUDA kernels for efficient inference. Skypilot also allows the deployment of vLLM endpoints on any instance in the cloud. # 2 Architectural details Mixtral is based on a transformer architecture [31] and uses the same modifications as described in [18], with the notable exceptions that Mix- tral supports a fully dense context length of 32k tokens, and the feed- forward blocks are replaced by Mixture-of-Expert layers (Section 2.1). The model architecture parameters are summarized in Table 1.\n",
      "\n",
      "This chunk describes the architectural details of the Mixtral language model, including its use of a transformer architecture with a 32k token context length and mixture-of-expert layers. It also mentions the model's open-source licensing and deployment options.\n"
     ]
    }
   ],
   "source": [
    "import numpy as np\n",
    "print(\"Top 3 Documents:\") \n",
    "for o in np.argsort(scores)[::-1][:3]:\n",
    "    print(retrieved_documents[o]['text'])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": ".venv",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.0"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
